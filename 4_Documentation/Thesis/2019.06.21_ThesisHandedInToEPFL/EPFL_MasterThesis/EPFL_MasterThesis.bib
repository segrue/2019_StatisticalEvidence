
@article{ioannidis_why_2005,
	title = {Why Most Published Research Findings Are False},
	volume = {2},
	issn = {1549-1676},
	url = {http://dx.plos.org/10.1371/journal.pmed.0020124},
	doi = {10.1371/journal.pmed.0020124},
	pages = {e124},
	number = {8},
	journaltitle = {{PLoS} Medicine},
	author = {Ioannidis, John P. A.},
	urldate = {2016-06-20},
	date = {2005-08-30},
	langid = {english},
	file = {Ionnaidis 2005_Why most published research findings are wrong.pdf:/home/drosoneuro/Zotero/storage/QT5ZGTQG/Ionnaidis 2005_Why most published research findings are wrong.pdf:application/pdf}
}

@article{ioannidis_exploratory_2007,
	title = {An exploratory test for an excess of significant findings},
	volume = {4},
	issn = {1740-7745},
	url = {http://ctj.sagepub.com/cgi/doi/10.1177/1740774507079441},
	doi = {10.1177/1740774507079441},
	pages = {245--253},
	number = {3},
	journaltitle = {Clinical Trials},
	author = {Ioannidis, J. P. and Trikalinos, T. A},
	urldate = {2017-02-19},
	date = {2007-06-01},
	langid = {english},
	keywords = {unread},
	file = {1740774507079441.pdf:/home/drosoneuro/Zotero/storage/UTAPGN89/1740774507079441.pdf:application/pdf}
}

@article{student_probable_1908,
	title = {The Probable Error of a Mean},
	volume = {6},
	issn = {0006-3444},
	url = {http://www.jstor.org/stable/2331554},
	pages = {1--25},
	number = {1},
	journaltitle = {Biometrika},
	author = {{Student}},
	urldate = {2017-05-07},
	date = {1908},
	file = {JSTOR Full Text PDF:/home/drosoneuro/Zotero/storage/B65E5CMU/Student - 1908 - The Probable Error of a Mean.pdf:application/pdf}
}

@article{devries_cumulative_2018,
	title = {The cumulative effect of reporting and citation biases on the apparent efficacy of treatments: the case of depression},
	issn = {0033-2917, 1469-8978},
	url = {https://www.cambridge.org/core/product/identifier/S0033291718001873/type/journal_article},
	doi = {10.1017/S0033291718001873},
	shorttitle = {The cumulative effect of reporting and citation biases on the apparent efficacy of treatments},
	pages = {1--3},
	journaltitle = {Psychological Medicine},
	author = {de Vries, Y. A. and Roest, A. M. and de Jonge, P. and Cuijpers, P. and Munafò, M. R. and Bastiaansen, J. A.},
	urldate = {2018-08-22},
	date = {2018-08-02},
	langid = {english},
	keywords = {unread},
	file = {de Vries et al. - 2018 - The cumulative effect of reporting and citation bi.pdf:/home/drosoneuro/Zotero/storage/QTBTYTHL/de Vries et al. - 2018 - The cumulative effect of reporting and citation bi.pdf:application/pdf}
}

@article{_reconciling_1987,
	title = {Reconciling Bayesian and Frequentist Evidence in the One-Sided Testing Problem},
	pages = {6},
	journaltitle = {Journal of the American Statistical Association},
	date = {1987},
	langid = {english},
	file = {1987 - Reconciling Bayesian and Frequentist Evidence in t.pdf:/home/drosoneuro/Zotero/storage/Q5935F2U/1987 - Reconciling Bayesian and Frequentist Evidence in t.pdf:application/pdf}
}

@article{berger_testing_1987,
	title = {Testing a Point Null Hypothesis: The Irreconcilability of P Values and Evidence},
	volume = {82},
	issn = {01621459},
	url = {https://www.jstor.org/stable/2289131?origin=crossref},
	doi = {10.2307/2289131},
	shorttitle = {Testing a Point Null Hypothesis},
	pages = {112},
	number = {397},
	journaltitle = {Journal of the American Statistical Association},
	author = {Berger, James O. and Sellke, Thomas},
	urldate = {2019-02-24},
	date = {1987-03},
	langid = {english},
	file = {Berger and Sellke - 1987 - Testing a Point Null Hypothesis The Irreconcilabi.pdf:/home/drosoneuro/Zotero/storage/2EQ7ZMFI/Berger and Sellke - 1987 - Testing a Point Null Hypothesis The Irreconcilabi.pdf:application/pdf}
}

@article{berger_could_2003,
	title = {Could Fisher, Jeffreys and Neyman Have Agreed on Testing?},
	volume = {18},
	issn = {0883-4237},
	url = {http://projecteuclid.org/euclid.ss/1056397485},
	doi = {10.1214/ss/1056397485},
	abstract = {Ronald Fisher advocated testing using p-values, Harold Jeffreys proposed use of objective posterior probabilities of hypotheses and Jerzy Neyman recommended testing with ﬁxed error probabilities. Each was quite critical of the other approaches. Most troubling for statistics and science is that the three approaches can lead to quite different practical conclusions.},
	pages = {1--32},
	number = {1},
	journaltitle = {Statistical Science},
	author = {Berger, James O.},
	urldate = {2019-02-24},
	date = {2003-02},
	langid = {english},
	keywords = {read},
	file = {Berger - 2003 - Could Fisher, Jeffreys and Neyman Have Agreed on T.pdf:/home/drosoneuro/Zotero/storage/G4XQAI99/Berger - 2003 - Could Fisher, Jeffreys and Neyman Have Agreed on T.pdf:application/pdf}
}

@article{bayarri_values_,
	title = {P Values for Composite Null Models},
	pages = {17},
	author = {Bayarri, M J},
	langid = {english},
	file = {Bayarri - P Values for Composite Null Models.pdf:/home/drosoneuro/Zotero/storage/KUDZ3UGZ/Bayarri - P Values for Composite Null Models.pdf:application/pdf}
}

@article{hayakawa_likelihood_,
	title = {The Likelihood Ratio Criterion for a Composite Hypothesis under a Local Alternative},
	abstract = {The asymptotic expansions of the distributions of the likelihood ratio criterion and Wald's statistic are derived for a composite hypothesis under a sequence of local alternative hypotheses converging to the null hypothesis when the sample size tends to infinity. Comparisons between the two statistics are made.},
	pages = {11},
	author = {Hayakawa, Takesi},
	langid = {english},
	file = {Hayakawa - The Likelihood Ratio Criterion for a Composite Hyp.pdf:/home/drosoneuro/Zotero/storage/RQ99SM49/Hayakawa - The Likelihood Ratio Criterion for a Composite Hyp.pdf:application/pdf}
}

@article{casella_reconciling_1987,
	title = {Reconciling Bayesian and Frequentist Evidence in the One-Sided Testing Problem},
	volume = {82},
	issn = {01621459},
	url = {https://www.jstor.org/stable/2289130?origin=crossref},
	doi = {10.2307/2289130},
	pages = {106},
	number = {397},
	journaltitle = {Journal of the American Statistical Association},
	author = {Casella, George and Berger, Roger L.},
	urldate = {2019-03-05},
	date = {1987-03},
	langid = {english},
	file = {Casella and Berger - 1987 - Reconciling Bayesian and Frequentist Evidence in t.pdf:/home/drosoneuro/Zotero/storage/U5Q3GBRV/Casella and Berger - 1987 - Reconciling Bayesian and Frequentist Evidence in t.pdf:application/pdf}
}

@article{efron_simultaneous_2008,
	title = {Simultaneous inference: When should hypothesis testing problems be combined?},
	volume = {2},
	issn = {1932-6157},
	url = {http://arxiv.org/abs/0803.3863},
	doi = {10.1214/07-AOAS141},
	shorttitle = {Simultaneous inference},
	abstract = {Modern statisticians are often presented with hundreds or thousands of hypothesis testing problems to evaluate at the same time, generated from new scientific technologies such as microarrays, medical and satellite imaging devices, or flow cytometry counters. The relevant statistical literature tends to begin with the tacit assumption that a single combined analysis, for instance, a False Discovery Rate assessment, should be applied to the entire set of problems at hand. This can be a dangerous assumption, as the examples in the paper show, leading to overly conservative or overly liberal conclusions within any particular subclass of the cases. A simple Bayesian theory yields a succinct description of the effects of separation or combination on false discovery rate analyses. The theory allows efficient testing within small subclasses, and has applications to ``enrichment,'' the detection of multi-case effects.},
	pages = {197--223},
	number = {1},
	journaltitle = {The Annals of Applied Statistics},
	author = {Efron, Bradley},
	urldate = {2019-03-19},
	date = {2008-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {0803.3863},
	keywords = {Statistics - Applications},
	file = {Efron - 2008 - Simultaneous inference When should hypothesis tes.pdf:/home/drosoneuro/Zotero/storage/3DVFTRHW/Efron - 2008 - Simultaneous inference When should hypothesis tes.pdf:application/pdf}
}

@article{bender_attention_2008,
	title = {Attention should be given to multiplicity issues in systematic reviews},
	volume = {61},
	issn = {08954356},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0895435608000930},
	doi = {10.1016/j.jclinepi.2008.03.004},
	abstract = {Objective: The objective of this paper is to describe the problem of multiple comparisons in systematic reviews and to provide some guidelines on how to deal with it in practice. Study Design and Setting: We describe common reasons for multiplicity in systematic reviews, and present some examples. We provide guidance on how to deal with multiplicity when it is unavoidable.
Results: We identiﬁed six common reasons for multiplicity in systematic reviews: multiple outcomes, multiple groups, multiple time points, multiple effect measures, subgroup analyses, and multiple looks at accumulating data. The existing methods to deal with multiplicity in single trials can not always be applied in systematic reviews.
Conclusion: There is no simple and completely satisfactory solution to the problem of multiple comparisons in systematic reviews. More research is required to develop multiple comparison procedures for use in systematic reviews. Authors and consumers of systematic reviews should give serious attention to multiplicity in systematic reviews when presenting, interpreting and using the results of these reports. Ó 2008 Elsevier Inc. All rights reserved.},
	pages = {857--865},
	number = {9},
	journaltitle = {Journal of Clinical Epidemiology},
	author = {Bender, Ralf and Bunce, Catey and Clarke, Mike and Gates, Simon and Lange, Stefan and Pace, Nathan L. and Thorlund, Kristian},
	urldate = {2019-04-01},
	date = {2008-09},
	langid = {english},
	file = {Bender et al. - 2008 - Attention should be given to multiplicity issues i.pdf:/home/drosoneuro/Zotero/storage/7T5U7US2/Bender et al. - 2008 - Attention should be given to multiplicity issues i.pdf:application/pdf}
}

@article{weed_metaanalysis_2010,
	title = {Meta-Analysis and Causal Inference: A Case Study of Benzene and Non-Hodgkin Lymphoma},
	volume = {20},
	issn = {10472797},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1047279710000207},
	doi = {10.1016/j.annepidem.2010.02.001},
	shorttitle = {Meta-Analysis and Causal Inference},
	abstract = {Meta-analysis is an important method in the practice of occupational epidemiology, with a legitimate, but limited role to play in causal inference. Meta-analysis provides an assessment of consistencydone of several classic causal criteriadthrough tests of heterogeneity and an assessment of differences across studies. It can also provide an increase in the precision of effect estimates, including the precision of dose response relationships. Causal inference, however, involves much more: a complete assessment of the classic causal criteria, for example. Causal claims, therefore, should not emerge from meta-analyses as such. A recent meta-analysis of epidemiological studies of benzene exposure and non-Hodgkin lymphoma ({NHL}), however, does exactly that. Using studies from a previous narrative review in which the authors made no causal claim, the same authors performed a meta-analysis and concluded that it represented new evidence that benzene causes {NHL}. Despite a lack of consistency (i.e., signiﬁcant heterogeneity), weak associations, no evidence of dose-response, no effort to provide an assessment of biological plausibility, and no new epidemiological evidence, the authors, nevertheless, changed their conclusion from association to causation. By using case study as an illustrative platform, this commentary provides cautionary and critical comments about the use of meta-analysis and causal inference in occupational epidemiology. Ann Epidemiol 2010;20:347–355. Ó 2010 Elsevier Inc. All rights reserved.},
	pages = {347--355},
	number = {5},
	journaltitle = {Annals of Epidemiology},
	author = {Weed, Douglas L.},
	urldate = {2019-04-02},
	date = {2010-05},
	langid = {english},
	file = {Weed - 2010 - Meta-Analysis and Causal Inference A Case Study o.pdf:/home/drosoneuro/Zotero/storage/NLD4L3RX/Weed - 2010 - Meta-Analysis and Causal Inference A Case Study o.pdf:application/pdf}
}

@article{efthimiou_combining_2017,
	title = {Combining randomized and non-randomized evidence in network meta-analysis: Combining randomized and non-randomized evidence in {NMA}},
	volume = {36},
	issn = {02776715},
	url = {http://doi.wiley.com/10.1002/sim.7223},
	doi = {10.1002/sim.7223},
	shorttitle = {Combining randomized and non-randomized evidence in network meta-analysis},
	pages = {1210--1226},
	number = {8},
	journaltitle = {Statistics in Medicine},
	author = {Efthimiou, Orestis and Mavridis, Dimitris and Debray, Thomas P. A. and Samara, Myrto and Belger, Mark and Siontis, George C. M. and Leucht, Stefan and Salanti, Georgia and {on behalf of GetReal Work Package 4}},
	urldate = {2019-04-05},
	date = {2017-04-15},
	langid = {english},
	file = {Efthimiou et al. - 2017 - Combining randomized and non-randomized evidence i.pdf:/home/drosoneuro/Zotero/storage/QRT976HT/Efthimiou et al. - 2017 - Combining randomized and non-randomized evidence i.pdf:application/pdf}
}

@article{dias_checking_2010,
	title = {Checking consistency in mixed treatment comparison meta-analysis},
	volume = {29},
	issn = {02776715, 10970258},
	url = {http://doi.wiley.com/10.1002/sim.3767},
	doi = {10.1002/sim.3767},
	pages = {932--944},
	number = {7},
	journaltitle = {Statistics in Medicine},
	author = {Dias, S. and Welton, N. J. and Caldwell, D. M. and Ades, A. E.},
	urldate = {2019-04-05},
	date = {2010-03-08},
	langid = {english},
	file = {Dias et al. - 2010 - Checking consistency in mixed treatment comparison.pdf:/home/drosoneuro/Zotero/storage/6MV4TXL6/Dias et al. - 2010 - Checking consistency in mixed treatment comparison.pdf:application/pdf}
}

@article{spiegelhalter_bayesian_2003,
	title = {Bayesian approaches to multiple sources of evidence and uncertainty in complex cost-effectiveness modelling},
	volume = {22},
	issn = {0277-6715, 1097-0258},
	url = {http://doi.wiley.com/10.1002/sim.1586},
	doi = {10.1002/sim.1586},
	abstract = {Increasingly complex models are being used to evaluate the cost-e ectiveness of medical interventions. We describe the multiple sources of uncertainty that are relevant to such models, and their relation to either probabilistic or deterministic sensitivity analysis. A Bayesian approach appears natural in this context. We explore how sensitivity analysis to patient heterogeneity and parameter uncertainty can be simultaneously investigated, and illustrate the necessary computation when expected costs and beneÿts can be calculated in closed form, such as in discrete-time discrete-state Markov models. Information about parameters can either be expressed as a prior distribution, or derived as a posterior distribution given a generalized synthesis of available data in which multiple sources of evidence can be di erentially weighted according to their assumed quality. The resulting joint posterior distributions on costs and beneÿts can then provide inferences on incremental cost-e ectiveness, best presented as posterior distributions over net-beneÿt and cost-e ectiveness acceptability curves. These ideas are illustrated with a detailed running example concerning the cost-e ectiveness of hip prostheses in di erent age–sex subgroups. All computations are carried out using freely available software for conducting Markov chain Monte Carlo analysis. Copyright ? 2003 John Wiley \& Sons, Ltd.},
	pages = {3687--3709},
	number = {23},
	journaltitle = {Statistics in Medicine},
	author = {Spiegelhalter, David J and Best, Nicola G},
	urldate = {2019-04-05},
	date = {2003-12-15},
	langid = {english},
	file = {Spiegelhalter and Best - 2003 - Bayesian approaches to multiple sources of evidenc.pdf:/home/drosoneuro/Zotero/storage/TRT2WTA9/Spiegelhalter and Best - 2003 - Bayesian approaches to multiple sources of evidenc.pdf:application/pdf}
}

@article{verde_combining_2015,
	title = {Combining randomized and non-randomized evidence in clinical research: a review of methods and applications: Combining Randomized and Non-Randomized Evidence},
	volume = {6},
	issn = {17592879},
	url = {http://doi.wiley.com/10.1002/jrsm.1122},
	doi = {10.1002/jrsm.1122},
	shorttitle = {Combining randomized and non-randomized evidence in clinical research},
	pages = {45--62},
	number = {1},
	journaltitle = {Research Synthesis Methods},
	author = {Verde, Pablo E. and Ohmann, Christian},
	urldate = {2019-04-05},
	date = {2015-03},
	langid = {english},
	file = {Verde and Ohmann - 2015 - Combining randomized and non-randomized evidence i.pdf:/home/drosoneuro/Zotero/storage/SAW85MM3/Verde and Ohmann - 2015 - Combining randomized and non-randomized evidence i.pdf:application/pdf}
}

@article{wang_bayesian_2019,
	title = {A Bayesian nonparametric causal inference model for synthesizing randomized clinical trial and real‐world evidence},
	issn = {0277-6715, 1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8134},
	doi = {10.1002/sim.8134},
	journaltitle = {Statistics in Medicine},
	author = {Wang, Chenguang and Rosner, Gary L.},
	urldate = {2019-04-05},
	date = {2019-03-18},
	langid = {english},
	file = {Wang and Rosner - 2019 - A Bayesian nonparametric causal inference model fo.pdf:/home/drosoneuro/Zotero/storage/88ET92VJ/Wang and Rosner - 2019 - A Bayesian nonparametric causal inference model fo.pdf:application/pdf}
}

@article{gonnermann_no_2015,
	title = {No solution yet for combining two independent studies in the presence of heterogeneity: {COMMENTARY}},
	volume = {34},
	issn = {02776715},
	url = {http://doi.wiley.com/10.1002/sim.6473},
	doi = {10.1002/sim.6473},
	shorttitle = {No solution yet for combining two independent studies in the presence of heterogeneity},
	pages = {2476--2480},
	number = {16},
	journaltitle = {Statistics in Medicine},
	author = {Gonnermann, Andrea and Framke, Theodor and Großhennig, Anika and Koch, Armin},
	urldate = {2019-04-05},
	date = {2015-07-20},
	langid = {english},
	file = {Gonnermann et al. - 2015 - No solution yet for combining two independent stud.pdf:/home/drosoneuro/Zotero/storage/V4RIHPGG/Gonnermann et al. - 2015 - No solution yet for combining two independent stud.pdf:application/pdf}
}

@article{verde_bayesian_2016,
	title = {Bayesian evidence synthesis for exploring generalizability of treatment effects: a case study of combining randomized and non-randomized results in diabetes: Bayesian evidence synthesis for exploring generalizability of treatment effects: a case study of combining randomized and non-randomized results in di},
	volume = {35},
	issn = {02776715},
	url = {http://doi.wiley.com/10.1002/sim.6809},
	doi = {10.1002/sim.6809},
	shorttitle = {Bayesian evidence synthesis for exploring generalizability of treatment effects},
	pages = {1654--1675},
	number = {10},
	journaltitle = {Statistics in Medicine},
	author = {Verde, Pablo E. and Ohmann, Christian and Morbach, Stephan and Icks, Andrea},
	urldate = {2019-04-05},
	date = {2016-05-10},
	langid = {english},
	file = {Verde et al. - 2016 - Bayesian evidence synthesis for exploring generali.pdf:/home/drosoneuro/Zotero/storage/3D5BUG5M/Verde et al. - 2016 - Bayesian evidence synthesis for exploring generali.pdf:application/pdf}
}

@article{watkins_prm230_2018,
	title = {{PRM}230 - A {SIMPLE} {NEW} {METHOD} {FOR} {COMBINING} {BINOMIAL} {COUNTS} {OR} {PROPORTIONS} {WITH} {HAZARD} {RATIOS} {FOR} {EVIDENCE} {SYNTHESIS} {OF} {TIME}-{TO}-{EVENT} {DATA}},
	volume = {21},
	issn = {10983015},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S109830151835650X},
	doi = {10.1016/j.jval.2018.09.2348},
	abstract = {{OBJECTIVES}: In a star-shaped network, comparative evaluations among treatments using only indirect evidences were based on an unidentiﬁable consistency assumption, limiting the reliability of the results. We suggest a data imputation method as a sensitivity analysis to assess robustness of the results depending on an unknown degree of consistency. {METHODS}: We attempted to ﬁll data for missing randomized controlled trials ({RCTs}) that may potentially be linked into the unclosed loop in a star-shaped network. A number of effect sizes and their standard errors were generated from a potential distribution of the unknown direct effect size. Starting from an assumption that the unknown direct results would be perfectly consistent with the existing indirect results, we compared deviance information criterions ({DIC}) in ﬁtted consistency and inconsistency models. The imputation was ﬁnalized when the {DIC} from the consistency model is close enough to that from the inconsistency model, suggesting a marginally acceptable level of overall inconsistency. We examined how the network metaanalysis ({NMA}) results after imputation could differ from that from the unclosed loop. For illustration, we took a closed loop data of streptokinase ({SK}), alteplase ({tPA}), and anistreptilase ({ASPAC}) from an existing network comparing thrombolytic drugs, and then eliminated data between {tPA} and {ASPAC}. The intentional missing data were re-imputed using the suggested method and the results were compared. {RESULTS}: The best drug to reduce mortality based on the original data where {RCTs} were present in all contrasts was {ASPAC}, followed by {tPA} and {SK}. When one contrast became disconnected, the ranking has changed to {tPA}, {ASPAC}, and {SK}. After the data imputation for the missing link, the results were coherent with that from the original closed loop where the result of inconsistency test was not signiﬁcant (p-valuez0.43). {CONCLUSIONS}: The data imputation method can be useful to help assessing reliability of results from a star-shaped {NMA}. {PRM}228 {EXTRAPOLATION} {OF} {SURVIVAL} {CURVES} {USING} {EXTERNAL} {INFORMATION}: {IMPLEMENTATION} {OF} {GUYOT}’S {METHOD} {IN} {PREVIOUSLY} {UNTREATED} {ADVANCED} {OR} {METASTATIC} {RENAL} {CELL} {CARCINOMA} Cawston H1, Genestier V1, Dale P2, Doan J3, Malcolm B2 1Amaris, Levallois-Perret, France, 2Bristol-Myers Squibb, Uxbridge, {UK}, 3Bristol-Myers Squibb, Princeton, {NJ}, {USA} {OBJECTIVES}: The extrapolation of survival is key when conducting cost-effectiveness analyses in oncology, especially when progression-Free ({PFS}) and overall survival ({OS}) outcomes have limited trial follow-up. Guidance from {NICE} suggest that the choice of distribution should be based on internal and external validation, the latter usually being undertaken by visual inspection. The aim of this study was to evaluate the impact of the use of the method developed by Guyot to include external data in the statistical extrapolation model itself, using survival outcomes from Checkmate 214 in 1L renal cell carcinoma ({RCC}).
{METHODS}: A systematic literature review on observational studies was conducted to identify relevant {OS} or {PFS} data for metastatic {RCC} patients with intermediate to poor prognosis. The assumption of the Guyot’s method is that after the trial follow-up, the conditional survival of the control arm should converge to the one of the external study. Parametric and cubic spline independent models were considered. Guyot’s method was subsequently implemented in a Bayesian framework to combine {CheckMate} 214 data with external information. {RESULTS}: Two studies reported long-term survival outcomes for intermediate to poor {mRCC} patients comparable to the ones from {CheckMate} 214. Based on the model with best ﬁt, Guyot’s method was found to have a moderate impact on results: for the sunitinib arm, the predicted {PFS} without external data adjustment (log-normal) was 25\% at 2 years, while it was 17\% with external data. At 5 years, the unadjusted estimate was 9\%, while it was reduced to 2\% including external data. A similar impact was observed on {OS}.
{CONCLUSIONS}: Guyot’s method is a useful tool for extrapolations of time-toevent data, as it adjusts long-term predictions rather than relying on a simple visual inspection. As the results are sensitive to the chosen external studies, choice of external data should be well justiﬁed.},
	pages = {S395},
	journaltitle = {Value in Health},
	author = {Watkins, Cl and Bennett, I},
	urldate = {2019-04-05},
	date = {2018-10},
	langid = {english},
	file = {Watkins and Bennett - 2018 - PRM230 - A SIMPLE NEW METHOD FOR COMBINING BINOMIA.pdf:/home/drosoneuro/Zotero/storage/F23RKURC/Watkins and Bennett - 2018 - PRM230 - A SIMPLE NEW METHOD FOR COMBINING BINOMIA.pdf:application/pdf}
}

@article{kulinskaya_combining_2010,
	title = {Combining the evidence using stable weights},
	volume = {1},
	issn = {17592879},
	url = {http://doi.wiley.com/10.1002/jrsm.20},
	doi = {10.1002/jrsm.20},
	pages = {284--296},
	number = {3},
	journaltitle = {Research Synthesis Methods},
	author = {Kulinskaya, Elena and Morgenthaler, Stephan and Staudte, Robert G.},
	urldate = {2019-04-05},
	date = {2010-07},
	langid = {english},
	file = {Kulinskaya et al. - 2010 - Combining the evidence using stable weights.pdf:/home/drosoneuro/Zotero/storage/D9976DT8/Kulinskaya et al. - 2010 - Combining the evidence using stable weights.pdf:application/pdf}
}

@article{lau_case_2006,
	title = {The case of the misleading funnel plot},
	volume = {333},
	issn = {0959-8138, 1468-5833},
	url = {http://www.bmj.com/lookup/doi/10.1136/bmj.333.7568.597},
	doi = {10.1136/bmj.333.7568.597},
	pages = {597--600},
	number = {7568},
	journaltitle = {{BMJ}},
	author = {Lau, Joseph and Ioannidis, John P A and Terrin, Norma and Schmid, Christopher H and Olkin, Ingram},
	urldate = {2019-05-14},
	date = {2006-09-16},
	langid = {english},
	file = {Lau et al. - 2006 - The case of the misleading funnel plot.pdf:/home/drosoneuro/Zotero/storage/ITBC7RAG/Lau et al. - 2006 - The case of the misleading funnel plot.pdf:application/pdf}
}

@article{mcshane_adjusting_2016,
	title = {Adjusting for Publication Bias in Meta-Analysis: An Evaluation of Selection Methods and Some Cautionary Notes},
	volume = {11},
	issn = {1745-6916, 1745-6924},
	url = {http://journals.sagepub.com/doi/10.1177/1745691616662243},
	doi = {10.1177/1745691616662243},
	shorttitle = {Adjusting for Publication Bias in Meta-Analysis},
	pages = {730--749},
	number = {5},
	journaltitle = {Perspectives on Psychological Science},
	author = {{McShane}, Blakeley B. and Böckenholt, Ulf and Hansen, Karsten T.},
	urldate = {2019-05-18},
	date = {2016-09},
	langid = {english},
	file = {McShane et al. - 2016 - Adjusting for Publication Bias in Meta-Analysis A.pdf:/home/drosoneuro/Zotero/storage/KH7EQKYJ/McShane et al. - 2016 - Adjusting for Publication Bias in Meta-Analysis A.pdf:application/pdf}
}

@article{andrews_identification_2017,
	title = {Identification of and correction for publication bias},
	volume = {w23298},
	abstract = {Some empirical results are more likely to be published than others. Such selective publication leads to biased estimates and distorted inference. This paper proposes two approaches for identifying the conditional probability of publication as a function of a study’s results, the ﬁrst based on systematic replication studies and the second based on meta-studies. For known conditional publication probabilities, we propose median-unbiased estimators and associated conﬁdence sets that correct for selective publication. We apply our methods to recent large-scale replication studies in experimental economics and psychology, and to meta-studies of the e↵ects of minimum wages and de-worming programs.},
	pages = {1--46},
	journaltitle = {National Bureau of Economic Research},
	author = {Andrews, Isaiah and Kasy, Maximilian},
	date = {2017},
	langid = {english},
	file = {Andrews - PublicationBias.pdf:/home/drosoneuro/Zotero/storage/GSSAU4JF/Andrews - PublicationBias.pdf:application/pdf;Andrews and Kasy - Identification of and correction for publication b.pdf:/home/drosoneuro/Zotero/storage/LKLQ727L/Andrews and Kasy - Identification of and correction for publication b.pdf:application/pdf;PublicationBiasSupplement.pdf:/home/drosoneuro/Zotero/storage/LMTT6GF8/PublicationBiasSupplement.pdf:application/pdf}
}

@article{duval_correcting_,
	title = {Correcting for Publication Bias in the Presence of Covariates},
	pages = {113},
	author = {Duval, Sue and Weinhandl, Eric},
	langid = {english},
	file = {Duval and Weinhandl - Correcting for Publication Bias in the Presence of.pdf:/home/drosoneuro/Zotero/storage/BWJMC5BT/Duval and Weinhandl - Correcting for Publication Bias in the Presence of.pdf:application/pdf}
}

@article{sutton_empirical_2000,
	title = {Empirical assessment of effect of publication bias on meta-analyses},
	volume = {320},
	issn = {09598138},
	url = {http://www.bmj.com/cgi/doi/10.1136/bmj.320.7249.1574},
	doi = {10.1136/bmj.320.7249.1574},
	abstract = {Objective To assess the effect of publication bias on the results and conclusions of systematic reviews and meta-analyses. Design Analysis of published meta-analyses by trim and fill method. Studies 48 reviews in Cochrane Database of Systematic Reviews that considered a binary endpoint and contained 10 or more individual studies. Main outcome measures Number of reviews with missing studies and effect on conclusions of meta-analyses.
Results The trim and fill fixed effects analysis method estimated that 26 (54\%) of reviews had missing studies and in 10 the number missing was significant. The corresponding figures with a random effects model were 23 (48\%) and eight. In four cases, statistical inferences regarding the effect of the intervention were changed after the overall estimate for publication bias was adjusted for.
Conclusions Publication or related biases were common within the sample of meta-analyses assessed. In most cases these biases did not affect the conclusions. Nevertheless, researchers should check routinely whether conclusions of systematic reviews are robust to possible non-random selection mechanisms.},
	pages = {1574--1577},
	number = {7249},
	journaltitle = {{BMJ}},
	author = {Sutton, A J},
	urldate = {2019-05-18},
	date = {2000-06-10},
	langid = {english},
	file = {Sutton - 2000 - Empirical assessment of effect of publication bias.pdf:/home/drosoneuro/Zotero/storage/DMZ5QSLQ/Sutton - 2000 - Empirical assessment of effect of publication bias.pdf:application/pdf}
}

@article{easterbrook_publication_1991,
	title = {Publication bias in clinical research},
	volume = {337},
	issn = {01406736},
	url = {https://linkinghub.elsevier.com/retrieve/pii/014067369190201Y},
	doi = {10.1016/0140-6736(91)90201-Y},
	pages = {867--872},
	number = {8746},
	journaltitle = {The Lancet},
	author = {Easterbrook, P.J and Gopalan, R and Berlin, J.A and Matthews, D.R},
	urldate = {2019-05-18},
	date = {1991-04},
	langid = {english},
	file = {Easterbrook et al. - 1991 - Publication bias in clinical research.pdf:/home/drosoneuro/Zotero/storage/8NYLX3RX/Easterbrook et al. - 1991 - Publication bias in clinical research.pdf:application/pdf}
}

@online{_identification_,
	title = {“Identification of and correction for publication bias,” and another discussion of how forking paths is not the same thing as file drawer « Statistical Modeling, Causal Inference, and Social Science},
	url = {https://statmodeling.stat.columbia.edu/2018/08/31/identification-correction-publication-bias-another-discussion-forking-paths-not-thing-file-drawer/},
	urldate = {2019-05-18},
	file = {“Identification of and correction for publication bias,” and another discussion of how forking paths is not the same thing as file drawer « Statistical Modeling, Causal Inference, and Social Science:/home/drosoneuro/Zotero/storage/32SIAYBW/identification-correction-publication-bias-another-discussion-forking-paths-not-thing-file-draw.html:text/html}
}

@online{_pcurve_,
	title = {The p-curve, p-uniform, and Hedges (1984) methods for meta-analysis under selection bias: An exchange with Blake {McShane}, Uri Simosohn, and Marcel van Assen « Statistical Modeling, Causal Inference, and Social Science},
	url = {https://statmodeling.stat.columbia.edu/2018/02/26/p-curve-p-uniform-hedges-1984-methods-meta-analysis-selection-bias-exchange-blake-mcshane-uri-simosohn/},
	urldate = {2019-05-18},
	file = {The p-curve, p-uniform, and Hedges (1984) methods for meta-analysis under selection bias\: An exchange with Blake McShane, Uri Simosohn, and Marcel van Assen « Statistical Modeling, Causal Inference, and Social Science:/home/drosoneuro/Zotero/storage/WD8QYB6T/p-curve-p-uniform-hedges-1984-methods-meta-analysis-selection-bias-exchange-blake-mcshane-uri-s.html:text/html}
}

@article{gelman_failure_2018,
	title = {The Failure of Null Hypothesis Significance Testing When Studying Incremental Changes, and What to Do About It},
	volume = {44},
	issn = {0146-1672, 1552-7433},
	url = {http://journals.sagepub.com/doi/10.1177/0146167217729162},
	doi = {10.1177/0146167217729162},
	abstract = {A standard mode of inference in social and behavioral science is to establish stylized facts using statistical signiﬁcance in quantitative studies. However, in a world in which measurements are noisy and eﬀects are small, this will not work: selection on statistical signiﬁcance leads to eﬀect sizes which are overestimated and often in the wrong direction. After a brief discussion of two examples, one in economics and one in social psychology, we consider the procedural solution of open post-publication review, the design solution of devoting more eﬀort to accurate measurements and within-person comparisons, and the statistical analysis solution of multilevel modeling and reporting all results rather than selection on signiﬁcance. We argue that the current replication crisis in science arises in part from the ill eﬀects of null hypothesis signiﬁcance testing being used to study small eﬀects with noisy data. In such settings, apparent success comes easy but truly replicable results require a more serious connection between theory, measurement, and data.},
	pages = {16--23},
	number = {1},
	journaltitle = {Personality and Social Psychology Bulletin},
	author = {Gelman, Andrew},
	urldate = {2019-05-18},
	date = {2018-01},
	langid = {english},
	file = {Gelman - 2018 - The Failure of Null Hypothesis Significance Testin.pdf:/home/drosoneuro/Zotero/storage/FKW28YNG/Gelman - 2018 - The Failure of Null Hypothesis Significance Testin.pdf:application/pdf}
}

@article{gelman_orbita_2019,
	title = {{ORBITA} and coronary stents: A case study in the analysis and reporting of clinical trials},
	issn = {00028703},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0002870319301000},
	doi = {10.1016/j.ahj.2019.04.011},
	shorttitle = {{ORBITA} and coronary stents},
	journaltitle = {American Heart Journal},
	author = {Gelman, Andrew and Carlin, John B. and Nallamothu, Brahmajee K},
	urldate = {2019-05-18},
	date = {2019-05},
	langid = {english},
	file = {Gelman et al. - 2019 - ORBITA and coronary stents A case study in the an.pdf:/home/drosoneuro/Zotero/storage/UUPF8NKG/Gelman et al. - 2019 - ORBITA and coronary stents A case study in the an.pdf:application/pdf}
}

@article{vanassen_metaanalysis_2015,
	title = {Meta-analysis using effect size distributions of only statistically significant studies.},
	volume = {20},
	issn = {1939-1463, 1082-989X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/met0000025},
	doi = {10.1037/met0000025},
	pages = {293--309},
	number = {3},
	journaltitle = {Psychological Methods},
	author = {van Assen, Marcel A. L. M. and van Aert, Robbie C. M. and Wicherts, Jelte M.},
	urldate = {2019-05-18},
	date = {2015},
	langid = {english},
	file = {van Assen et al. - 2015 - Meta-analysis using effect size distributions of o.pdf:/home/drosoneuro/Zotero/storage/NDAIFUHE/van Assen et al. - 2015 - Meta-analysis using effect size distributions of o.pdf:application/pdf}
}

@article{hedges_estimation_1984,
	title = {Estimation of Effect Size under Nonrandom Sampling: The Effects of Censoring Studies Yielding Statistically Insignificant Mean Differences},
	volume = {9},
	issn = {03629791},
	url = {https://www.jstor.org/stable/1164832?origin=crossref},
	doi = {10.2307/1164832},
	shorttitle = {Estimation of Effect Size under Nonrandom Sampling},
	abstract = {Quantitative research synthesis usually involves the combination of estimates of the standardized mean difference (effect size) derived from independent research studies. In some cases, effect size estimates are available only if the difference between experimental and control group means is statistically significant. If the quantitative result of a study is observed only when the mean difference is statistically significant, the observed mean difference, variance, and effect size are biased estimators of the corresponding population parameters. The exact distribution of the sample effect size is derived for the case in which only studies yielding statistically significant results may be observed. The maximum likelihood estimator of effect size also is derived under the model in which only significant results are observed. The exact distribution of the maximum likelihood estimator is obtained numerically and is used to study the bias of the maximum likelihood estimator. An empirical sampling study is used to supplement the analytic results.},
	pages = {61},
	number = {1},
	journaltitle = {Journal of Educational Statistics},
	author = {Hedges, Larry V.},
	urldate = {2019-05-18},
	date = {1984},
	langid = {english},
	file = {Hedges - 1984 - Estimation of Effect Size under Nonrandom Sampling.pdf:/home/drosoneuro/Zotero/storage/9WUTXKKG/Hedges - 1984 - Estimation of Effect Size under Nonrandom Sampling.pdf:application/pdf}
}

@article{rosenthal_file_1979,
	title = {The "File Drawer Problem" and Tolerance for Null Results},
	volume = {86},
	abstract = {For  any  given  research area,  one  cannot  tell  how  many  studies  have  been con-ducted  but  never  reported.  The  extreme  view  of  the  "file  drawer  problem"  isthat  journals  are  filled  with  the  5\%  of  the  studies  that  show  Type  I  errors,while  the  file  drawers  are  filled  with  the  95\%  of  the  studies  that  show non-significant  results.  Quantitative  procedures  for  computing  the  tolerance  for filedand  future   null  results  are  reported  and  illustrated,  and  the  implications  arediscussed},
	pages = {638--641},
	number = {3},
	journaltitle = {Psychological Bulletin},
	author = {Rosenthal, Robert},
	date = {1979},
	file = {rosenthal1979.pdf:/home/drosoneuro/Zotero/storage/X3UJUM8U/rosenthal1979.pdf:application/pdf}
}

@article{egger_bias_1997,
	title = {Bias in meta-analysis detected by a simple, graphical test},
	volume = {315},
	issn = {0959-8138, 1468-5833},
	url = {http://www.bmj.com/cgi/doi/10.1136/bmj.315.7109.629},
	doi = {10.1136/bmj.315.7109.629},
	abstract = {Objective: Funnel plots (plots of effect estimates against sample size) may be useful to detect bias in meta-analyses that were later contradicted by large trials. We examined whether a simple test of asymmetry of funnel plots predicts discordance of results when meta-analyses are compared to large trials, and we assessed the prevalence of bias in published meta-analyses. Design: Medline search to identify pairs consisting of a meta-analysis and a single large trial (concordance of results was assumed if effects were in the same direction and the meta-analytic estimate was within 30\% of the trial); analysis of funnel plots from 37 meta-analyses identified from a hand search of four leading general medicine journals 1993-6 and 38 meta-analyses from the second 1996 issue of the Cochrane Database of Systematic Reviews. Main outcome measure: Degree of funnel plot asymmetry as measured by the intercept from regression of standard normal deviates against precision.
Results: In the eight pairs of meta-analysis and large trial that were identified (five from cardiovascular medicine, one from diabetic medicine, one from geriatric medicine, one from perinatal medicine) there were four concordant and four discordant pairs. In all cases discordance was due to meta-analyses showing larger effects. Funnel plot asymmetry was present in three out of four discordant pairs but in none of concordant pairs. In 14 (38\%) journal meta-analyses and 5 (13\%) Cochrane reviews, funnel plot asymmetry indicated that there was bias.
Conclusions: A simple analysis of funnel plots provides a useful test for the likely presence of bias in meta-analyses, but as the capacity to detect bias will be limited when meta-analyses are based on a limited number of small trials the results from such analyses should be treated with considerable caution.},
	pages = {629--634},
	number = {7109},
	journaltitle = {{BMJ}},
	author = {Egger, M. and Smith, G. D. and Schneider, M. and Minder, C.},
	urldate = {2019-05-20},
	date = {1997-09-13},
	langid = {english},
	file = {Egger et al. - 1997 - Bias in meta-analysis detected by a simple, graphi.pdf:/home/drosoneuro/Zotero/storage/F9Z73TUI/Egger et al. - 1997 - Bias in meta-analysis detected by a simple, graphi.pdf:application/pdf}
}

@article{mcdaniel_publication_2006,
	title = {{PUBLICATION} {BIAS}: A {CASE} {STUDY} {OF} {FOUR} {TEST} {VENDORS}},
	volume = {59},
	issn = {0031-5826, 1744-6570},
	url = {http://doi.wiley.com/10.1111/j.1744-6570.2006.00059.x},
	doi = {10.1111/j.1744-6570.2006.00059.x},
	shorttitle = {{PUBLICATION} {BIAS}},
	pages = {927--953},
	number = {4},
	journaltitle = {Personnel Psychology},
	author = {{McDANIEL}, Michael A. and Rothstein, Hannah R. and Whetzel, Deborah L.},
	urldate = {2019-05-20},
	date = {2006-12},
	langid = {english},
	file = {McDANIEL et al. - 2006 - PUBLICATION BIAS A CASE STUDY OF FOUR TEST VENDOR.pdf:/home/drosoneuro/Zotero/storage/D62MDMKK/McDANIEL et al. - 2006 - PUBLICATION BIAS A CASE STUDY OF FOUR TEST VENDOR.pdf:application/pdf}
}

@article{duval_trim_2000,
	title = {Trim and Fill: A Simple Funnel-Plot-Based Method of Testing and Adjusting for Publication Bias in Meta-Analysis},
	volume = {56},
	issn = {0006341X},
	url = {http://doi.wiley.com/10.1111/j.0006-341X.2000.00455.x},
	doi = {10.1111/j.0006-341X.2000.00455.x},
	shorttitle = {Trim and Fill},
	abstract = {We study recently developed nonparametric methods for estimating the number of missing studies that might exist in a meta-analysis and the effect that these studies might have had on its outcome. These are simple rank-based data augmentation techniques, which formalize the use of funnel plots. We show that they provide effective and relatively powerful tests for evaluating the existence of such publication bias. Af'ter adjusting for missing studies, we find that the point estimate of the overall effect size is approximately correct and coverage of the effect size confidence intervals is substantially improved, in many cases recovering the nominal confidence levels entirely. We illustrate the trim and fill method on existing meta-analyses of studies in clinical trials and psychometrics.},
	pages = {455--463},
	number = {2},
	journaltitle = {Biometrics},
	author = {Duval, Sue and Tweedie, Richard},
	urldate = {2019-05-20},
	date = {2000-06},
	langid = {english},
	file = {Duval and Tweedie - 2000 - Trim and Fill A Simple Funnel-Plot-Based Method o.pdf:/home/drosoneuro/Zotero/storage/IQZ9E8NE/Duval and Tweedie - 2000 - Trim and Fill A Simple Funnel-Plot-Based Method o.pdf:application/pdf}
}

@article{mccrary_conservative_2016,
	title = {Conservative Tests under Satisficing Models of Publication Bias},
	volume = {11},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0149590},
	doi = {10.1371/journal.pone.0149590},
	pages = {e0149590},
	number = {2},
	journaltitle = {{PLOS} {ONE}},
	author = {{McCrary}, Justin and Christensen, Garret and Fanelli, Daniele},
	editor = {Marinazzo, Daniele},
	urldate = {2019-05-20},
	date = {2016-02-22},
	langid = {english},
	file = {McCrary et al. - 2016 - Conservative Tests under Satisficing Models of Pub.PDF:/home/drosoneuro/Zotero/storage/5RCGHRNJ/McCrary et al. - 2016 - Conservative Tests under Satisficing Models of Pub.PDF:application/pdf}
}

@article{card_timeseries_,
	title = {Time-Series Minimum-Wage Studies: A Meta-analysis},
	pages = {7},
	author = {Card, David and Krueger, Alan B},
	langid = {english},
	file = {Card and Krueger - Time-Series Minimum-Wage Studies A Meta-analysis.pdf:/home/drosoneuro/Zotero/storage/H5ZFDULI/Card and Krueger - Time-Series Minimum-Wage Studies A Meta-analysis.pdf:application/pdf}
}

@article{ning_maximum_2017,
	title = {Maximum likelihood estimation and {EM} algorithm of Copas-like selection model for publication bias correction},
	volume = {18},
	issn = {1465-4644, 1468-4357},
	url = {https://academic.oup.com/biostatistics/article/18/3/495/3056196},
	doi = {10.1093/biostatistics/kxx004},
	abstract = {Publication bias occurs when the published research results are systematically unrepresentative of the population of studies that have been conducted, and is a potential threat to meaningful meta-analysis. The Copas selection model provides a ﬂexible framework for correcting estimates and offers considerable insight into the publication bias. However, maximizing the observed likelihood under the Copas selection model is challenging because the observed data contain very little information on the latent variable. In this article, we study a Copas-like selection model and propose an expectation-maximization ({EM}) algorithm for estimation based on the full likelihood. Empirical simulation studies show that the {EM} algorithm and its associated inferential procedure performs well and avoids the non-convergence problem when maximizing the observed likelihood.},
	pages = {495--504},
	number = {3},
	journaltitle = {Biostatistics},
	author = {Ning, Jing and Chen, Yong and Piao, Jin},
	urldate = {2019-05-27},
	date = {2017-07-01},
	langid = {english},
	file = {Ning et al. - 2017 - Maximum likelihood estimation and EM algorithm of .pdf:/home/drosoneuro/Zotero/storage/UEQP3BU3/Ning et al. - 2017 - Maximum likelihood estimation and EM algorithm of .pdf:application/pdf}
}

@article{schwarzer_empirical_2010,
	title = {Empirical evaluation suggests Copas selection model preferable to trim-and-fill method for selection bias in meta-analysis},
	volume = {63},
	issn = {08954356},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0895435609002236},
	doi = {10.1016/j.jclinepi.2009.05.008},
	abstract = {Objective: Meta-analysis yields a biased result if published studies represent a biased selection of the evidence. Copas proposed a selection model to assess the sensitivity of meta-analysis conclusions to possible selection bias. An alternative proposal is the trimand-ﬁll method. This article reports an empirical comparison of the two methods. Study Design and Setting: We took 157 meta-analyses with binary outcomes, analyzed each one using both methods, then performed an automated comparison of the results. We compared the treatment estimates, standard errors, associated P-values, and number of missing studies estimated by both methods.
Results: Both methods give similar point estimates, but standard errors and P-values are systematically larger for the trim-and-ﬁll method. Furthermore, P-values from the trim-and-ﬁll method are typically larger than those from the usual random effects model when no selection bias is detected. By contrast, P-values from the Copas selection model and the usual random effects model are similar in this setting. The trim-and-ﬁll method reports more missing studies than the Copas selection model, unless selection bias is detected when the position is reversed.
Conclusions: The assumption that the most extreme studies are missing leads to excessively conservative inference in practice for the trim-and-ﬁll method. The Copas selection model appears to be the preferable approach. Ó 2010 Elsevier Inc. All rights reserved.},
	pages = {282--288},
	number = {3},
	journaltitle = {Journal of Clinical Epidemiology},
	author = {Schwarzer, Guido and Carpenter, James and Rücker, Gerta},
	urldate = {2019-05-27},
	date = {2010-03},
	langid = {english},
	file = {Schwarzer et al. - 2010 - Empirical evaluation suggests Copas selection mode.pdf:/home/drosoneuro/Zotero/storage/C7EZIG7D/Schwarzer et al. - 2010 - Empirical evaluation suggests Copas selection mode.pdf:application/pdf}
}

@article{piao_copaslike_2018,
	title = {Copas-like selection model to correct publication bias in systematic review of diagnostic test studies},
	issn = {0962-2802, 1477-0334},
	url = {http://journals.sagepub.com/doi/10.1177/0962280218791602},
	doi = {10.1177/0962280218791602},
	abstract = {The accuracy of a diagnostic test, which is often quantified by a pair of measures such as sensitivity and specificity, is critical for medical decision making. Separate studies of an investigational diagnostic test can be combined through metaanalysis; however, such an analysis can be threatened by publication bias. To the best of our knowledge, there is no existing method that accounts for publication bias in the meta-analysis of diagnostic tests involving bivariate outcomes. In this paper, we extend the Copas selection model from univariate outcomes to bivariate outcomes for the correction of publication bias when the probability of a study being published can depend on its sensitivity, specificity, and the associated standard errors. We develop an expectation-maximization algorithm for the maximum likelihood estimation under the proposed selection model. We investigate the finite sample performance of the proposed method through simulation studies and illustrate the method by assessing a meta-analysis of 17 published studies of a rapid diagnostic test for influenza.},
	pages = {096228021879160},
	journaltitle = {Statistical Methods in Medical Research},
	author = {Piao, Jin and Liu, Yulun and Chen, Yong and Ning, Jing},
	urldate = {2019-05-27},
	date = {2018-07-31},
	langid = {english},
	file = {Piao et al. - 2018 - Copas-like selection model to correct publication .pdf:/home/drosoneuro/Zotero/storage/4IWD4M9A/Piao et al. - 2018 - Copas-like selection model to correct publication .pdf:application/pdf}
}

@article{carpenter_copas_2009,
	title = {copas: An R package for Fitting the Copas Selection Model},
	volume = {1},
	issn = {2073-4859},
	url = {https://journal.r-project.org/archive/2009/RJ-2009-012/index.html},
	doi = {10.32614/RJ-2009-012},
	shorttitle = {copas},
	abstract = {This article describes the R package copas which is an add-on package to the R package meta. The R package copas can be used to ﬁt the Copas selection model to adjust for bias in meta-analysis. A clinical example is used to illustrate ﬁtting and interpreting the Copas selection model.},
	pages = {31},
	number = {2},
	journaltitle = {The R Journal},
	author = {Carpenter, J. and Rücker, G. and Schwarzer, G.},
	urldate = {2019-05-27},
	date = {2009},
	langid = {english},
	file = {Carpenter et al. - 2009 - copas An R package for Fitting the Copas Selectio.pdf:/home/drosoneuro/Zotero/storage/ZKGATYTK/Carpenter et al. - 2009 - copas An R package for Fitting the Copas Selectio.pdf:application/pdf}
}

@book{rothstein_publication_2005,
	title = {Publication bias in meta-analysis: Prevention, assessment and adjustments},
	volume = {72},
	url = {http://link.springer.com/10.1007/s11336-006-1450-y},
	shorttitle = {Publication bias in meta-analysis},
	author = {Rothstein, Hannah R. and Sutton, Alexander J. and Borenstein, Michae},
	urldate = {2019-05-27},
	date = {2005},
	langid = {english},
	file = {Rothstein et al. - 2005 - Publication bias in meta-analysis Prevention, ass.pdf:/home/drosoneuro/Zotero/storage/EE34TV7U/Rothstein et al. - 2005 - Publication bias in meta-analysis Prevention, ass.pdf:application/pdf}
}

@article{sterling_publication_1959,
	title = {Publication Decisions and their Possible Effects on Inferences Drawn from Tests of Significance—or Vice Versa},
	volume = {54},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1959.10501497},
	doi = {10.1080/01621459.1959.10501497},
	pages = {30--34},
	number = {285},
	journaltitle = {Journal of the American Statistical Association},
	author = {Sterling, Theodore D.},
	urldate = {2019-06-01},
	date = {1959-03},
	langid = {english},
	file = {Sterling - 1959 - Publication Decisions and their Possible Effects o.pdf:/home/drosoneuro/Zotero/storage/6L5CBYG4/Sterling - 1959 - Publication Decisions and their Possible Effects o.pdf:application/pdf}
}

@article{dickersin_existence_1990,
	title = {The existence of publication bias and risk factors for its occurrence},
	volume = {263},
	issn = {00987484, 15383598},
	url = {http://jama.ama-assn.org/cgi/doi/10.1001/jama.263.10.1385},
	doi = {10.1001/jama.263.10.1385},
	pages = {1385--1389},
	number = {10},
	journaltitle = {{JAMA}: The Journal of the American Medical Association},
	author = {Dickersin, K.},
	urldate = {2019-06-01},
	date = {1990-03-09},
	langid = {english},
	keywords = {read},
	file = {Dickersin - 1990 - The existence of publication bias and risk factors.pdf:/home/drosoneuro/Zotero/storage/S6LQPM4P/Dickersin - 1990 - The existence of publication bias and risk factors.pdf:application/pdf}
}

@article{franco_publication_2014,
	title = {Publication bias in the social sciences: Unlocking the file drawer},
	volume = {345},
	issn = {0036-8075, 1095-9203},
	url = {http://www.sciencemag.org/cgi/doi/10.1126/science.1255484},
	doi = {10.1126/science.1255484},
	shorttitle = {Publication bias in the social sciences},
	pages = {1502--1505},
	number = {6203},
	journaltitle = {Science},
	author = {Franco, A. and Malhotra, N. and Simonovits, G.},
	urldate = {2019-06-02},
	date = {2014-09-19},
	langid = {english},
	keywords = {read},
	file = {Franco et al. - 2014 - Publication bias in the social sciences Unlocking.pdf:/home/drosoneuro/Zotero/storage/6XWTHW5G/Franco et al. - 2014 - Publication bias in the social sciences Unlocking.pdf:application/pdf}
}

@article{cooper_finding_1997,
	title = {Finding the missing science: The fate of studies submitted for review by a human subjects committee.},
	volume = {2},
	issn = {1082-989X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/1082-989X.2.4.447},
	doi = {10.1037/1082-989X.2.4.447},
	shorttitle = {Finding the missing science},
	pages = {447--452},
	number = {4},
	journaltitle = {Psychological Methods},
	shortjournal = {Psychological Methods},
	author = {Cooper, Harris and {DeNeve}, Kristina and Charlton, Kelly},
	urldate = {2019-06-02},
	date = {1997},
	langid = {english},
	keywords = {read},
	file = {Full Text:/home/drosoneuro/Zotero/storage/YGL3QE6G/Cooper et al. - 1997 - Finding the missing science The fate of studies s.pdf:application/pdf}
}

@article{begg_operating_1994,
	title = {Operating Characteristics of a Rank Correlation Test for Publication Bias},
	volume = {50},
	issn = {0006341X},
	url = {https://www.jstor.org/stable/2533446?origin=crossref},
	doi = {10.2307/2533446},
	abstract = {An adjusted rank correlation test is proposed as a technique for identifying publication bias in a meta-analysis, and its operating characteristics are evaluated via simulations. The test statistic is a direct statistical analogue of the popular "funnel-graph." The number of component studies in the meta-analysis, the nature of the selection mechanism, the range of variances of the effect size estimates, and the true underlying effect size are all observed to be influential in determining the power of the test. The test is fairly powerful for large meta-analyses with 75 component studies, but has only moderate power for meta-analyses with 25 component studies. However, in many of the configurations in which there is low power, there is also relatively little bias in the summary effect size estimate. Nonetheless, the test must be interpreted with caution in small meta-analyses. In particular, bias cannot be ruled out if the test is not significant. The proposed technique has potential utility as an exploratory tool for meta-analysts, as a formal procedure to complement the funnelgraph.},
	pages = {1088},
	number = {4},
	journaltitle = {Biometrics},
	author = {Begg, Colin B. and Mazumdar, Madhuchhanda},
	urldate = {2019-06-02},
	date = {1994-12},
	langid = {english},
	file = {Begg and Mazumdar - 1994 - Operating Characteristics of a Rank Correlation Te.pdf:/home/drosoneuro/Zotero/storage/6JRKVSCS/Begg and Mazumdar - 1994 - Operating Characteristics of a Rank Correlation Te.pdf:application/pdf}
}

@article{chalmers_minimizing_1990,
	title = {Minimizing the Three Stages of Publication Bias},
	volume = {263},
	pages = {1392--1395},
	number = {10},
	journaltitle = {{JAMA}},
	author = {Chalmers, Thomas C. and Frank, Cynthia S. and Reitman, Dinah},
	date = {1990},
	keywords = {read},
	file = {chalmers1990.pdf:/home/drosoneuro/Zotero/storage/W5HKJN9A/chalmers1990.pdf:application/pdf}
}

@article{antonakis_doing_2017,
	title = {On doing better science: From thrill of discovery to policy implications},
	volume = {28},
	issn = {10489843},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S104898431730070X},
	doi = {10.1016/j.leaqua.2017.01.006},
	shorttitle = {On doing better science},
	abstract = {In this position paper, I argue that the main purpose of research is to discover and report on phenomena in a truthful manner. Once uncovered, these phenomena can have important implications for society. The utility of research depends on whether it makes a contribution because it is original or can add to cumulative research efforts, is rigorously and reliably done, and is able to inform basic or applied research and later policy. However, ﬁve serious “diseases” stiﬂe the production of useful research. These diseases include: signiﬁcosis, an inordinate focus on statistically signiﬁcant results; neophilia, an excessive appreciation for novelty; theorrhea, a mania for new theory; arigorium, a deﬁciency of rigor in theoretical and empirical work; and ﬁnally, disjunctivitis, a proclivity to produce large quantities of redundant, trivial, and incoherent works. I surmise that these diseases have caused immense harm to science and have cast doubt on the role of science in society. I discuss what publication gatekeepers should do to eradicate these diseases, to stimulate the undertaking of more useful and impactful research, and to provide the needed incentives to better align the interests of researchers with those of the greater good. Finally, I highlight where technical improvements are needed to enhance research quality, and call on deeper reﬂection, transparency, and honesty in how we do research.},
	pages = {5--21},
	number = {1},
	journaltitle = {The Leadership Quarterly},
	author = {Antonakis, John},
	urldate = {2019-06-02},
	date = {2017-02},
	langid = {english},
	file = {Antonakis - 2017 - On doing better science From thrill of discovery .pdf:/home/drosoneuro/Zotero/storage/ADVPWYVP/Antonakis - 2017 - On doing better science From thrill of discovery .pdf:application/pdf}
}

@article{mcshane_abandon_2019,
	title = {Abandon Statistical Significance},
	volume = {73},
	issn = {0003-1305, 1537-2731},
	url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1527253},
	doi = {10.1080/00031305.2018.1527253},
	abstract = {We discuss problems the null hypothesis significance testing ({NHST}) paradigm poses for replication and more broadly in the biomedical and social sciences as well as how these problems remain unresolved by proposals involving modified p-value thresholds, confidence intervals, and Bayes factors. We then discuss our own proposal, which is to abandon statistical significance. We recommend dropping the {NHST} paradigm—and the p-value thresholds intrinsic to it—as the default statistical paradigm for research, publication, and discovery in the biomedical and social sciences. Specifically, we propose that the p-value be demoted from its threshold screening role and instead, treated continuously, be considered along with currently subordinate factors (e.g., related prior evidence, plausibility of mechanism, study design and data quality, real world costs and benefits, novelty of finding, and other factors that vary by research domain) as just one among many pieces of evidence. We have no desire to “ban” p-values or other purely statistical measures. Rather, we believe that such measures should not be thresholded and that, thresholded or not, they should not take priority over the currently subordinate factors. We also argue that it seldom makes sense to calibrate evidence as a function of p-values or other purely statistical measures. We offer recommendations for how our proposal can be implemented in the scientific publication process as well as in statistical decision making more broadly.},
	pages = {235--245},
	issue = {sup1},
	journaltitle = {The American Statistician},
	author = {{McShane}, Blakeley B. and Gal, David and Gelman, Andrew and Robert, Christian and Tackett, Jennifer L.},
	urldate = {2019-06-03},
	date = {2019-03-29},
	langid = {english},
	file = {McShane et al. - 2019 - Abandon Statistical Significance.pdf:/home/drosoneuro/Zotero/storage/JKW67U7J/McShane et al. - 2019 - Abandon Statistical Significance.pdf:application/pdf}
}

@article{amrhein_inferential_2019,
	title = {Inferential Statistics as Descriptive Statistics: There Is No Replication Crisis if We Don’t Expect Replication},
	volume = {73},
	abstract = {Statistical inference often fails to replicate. One reason is that many results may be selected for drawing inference because some threshold of a statistic like the P-value was crossed, leading to biased reported effect sizes. Nonetheless, considerable non-replication is to be expected even without selective reporting, and generalizations from single studies are rarely if ever warranted. Honestly reported results must vary from replication to replication because of varying assumption violations and random variation; excessive agreement itself would suggest deeper problems, such as failure to publish results in conflict with group expectations or desires. A general perception of a “replication crisis” may thus reflect failure to recognize that statistical tests not only test hypotheses, but countless assumptions and the entire environment in which research takes place. Because of all the uncertain and unknown assumptions that underpin statistical inferences, we should treat inferential statistics as highly unstable local descriptions of relations between assumptions and data, rather than as providing generalizable inferences about hypotheses or models. And that means we should treat statistical results as being much more incomplete and uncertain than is currently the norm. Acknowledging this uncertainty could help reduce the allure of selective reporting: Since a small P-value could be large in a replication study, and a large P-value could be small, there is simply no need to selectively report studies based on statistical results. Rather than focusing our study reports on uncertain conclusions, we should thus focus on describing accurately how the study was conducted, what problems occurred, what data were obtained, what analysis methods were used and why, and what output those methods produced.},
	pages = {262--270},
	number = {51},
	journaltitle = {{THE} {AMERICAN} {STATISTICIAN}},
	author = {Amrhein, Valentin and Trafimow, David and Greenland, Sander},
	date = {2019},
	langid = {english},
	file = {Amrhein et al. - Inferential Statistics as Descriptive Statistics .pdf:/home/drosoneuro/Zotero/storage/P9SZBPIE/Amrhein et al. - Inferential Statistics as Descriptive Statistics .pdf:application/pdf}
}

@article{ioannidis_importance_2019,
	title = {The Importance of Predefined Rules and Prespecified Statistical Analyses: Do Not Abandon Significance},
	issn = {0098-7484},
	url = {http://jama.jamanetwork.com/article.aspx?doi=10.1001/jama.2019.4582},
	doi = {10.1001/jama.2019.4582},
	shorttitle = {The Importance of Predefined Rules and Prespecified Statistical Analyses},
	journaltitle = {{JAMA}},
	author = {Ioannidis, John P. A.},
	urldate = {2019-06-03},
	date = {2019-04-04},
	langid = {english},
	file = {Ioannidis - 2019 - The Importance of Predefined Rules and Prespecifie.pdf:/home/drosoneuro/Zotero/storage/HRMVR2Q9/Ioannidis - 2019 - The Importance of Predefined Rules and Prespecifie.pdf:application/pdf}
}

@article{gerber_publication_2008,
	title = {Publication Bias in Empirical Sociological Research: Do Arbitrary Significance Levels Distort Published Results?},
	volume = {37},
	issn = {0049-1241, 1552-8294},
	url = {http://journals.sagepub.com/doi/10.1177/0049124108318973},
	doi = {10.1177/0049124108318973},
	shorttitle = {Publication Bias in Empirical Sociological Research},
	pages = {3--30},
	number = {1},
	journaltitle = {Sociological Methods \& Research},
	author = {Gerber, Alan S. and Malhotra, Neil},
	urldate = {2019-06-03},
	date = {2008-08},
	langid = {english},
	file = {Gerber and Malhotra - 2008 - Publication Bias in Empirical Sociological Researc.pdf:/home/drosoneuro/Zotero/storage/9RSYAICM/Gerber and Malhotra - 2008 - Publication Bias in Empirical Sociological Researc.pdf:application/pdf}
}

@article{amrhein_retire_2019,
	title = {Retire statistical significance},
	pages = {3},
	author = {Amrhein, Valentin and Greenland, Sander and {McShane}, Blake},
	date = {2019},
	langid = {english},
	file = {Amrhein et al. - Retire statistical significance.pdf:/home/drosoneuro/Zotero/storage/UM7AAEKB/Amrhein et al. - Retire statistical significance.pdf:application/pdf}
}

@article{neyman_testing_1933,
	title = {The testing of statistical hypotheses in relation to probabilities a priori},
	volume = {29},
	issn = {0305-0041, 1469-8064},
	url = {http://www.journals.cambridge.org/abstract_S030500410001152X},
	doi = {10.1017/S030500410001152X},
	pages = {492},
	number = {4},
	journaltitle = {Mathematical Proceedings of the Cambridge Philosophical Society},
	author = {Neyman, J. and Pearson, E. S. and Yule, G. U.},
	urldate = {2019-06-03},
	date = {1933-10},
	langid = {english},
	file = {Neyman et al. - 1933 - The testing of statistical hypotheses in relation .pdf:/home/drosoneuro/Zotero/storage/B3T9IGA9/Neyman et al. - 1933 - The testing of statistical hypotheses in relation .pdf:application/pdf}
}

@book{fisher_statistical_1925,
	title = {Statistical methods for research workers},
	volume = {6},
	publisher = {Oliver and Boyd},
	author = {Fisher, Ronald A},
	date = {1925},
	file = {[R.A._(Sir_Ronald)_Fisher]_Statistical_methods_for(z-lib.org).djvu:/home/drosoneuro/Zotero/storage/ZFW2NLAI/[R.A._(Sir_Ronald)_Fisher]_Statistical_methods_for(z-lib.org).djvu:image/vnd.djvu}
}

@article{cowles_origins_1982,
	title = {On the Origins of the .05 Level of Statistical Significance},
	abstract = {Examination of the literature in statistics and probability that predates Fisher's Statistical Methods for Research Workers indicates that although Fisher is responsible for the first formal statement of the .05 criterion for statistical significance, the concept goes back much further. The move toward conventional levels for the rejection of the hypothesis of chance dates from the turn of the century. Early statements about statistical significance were given in terms of the probable error. These earlier conventions were adopted and restated by Fisher.},
	pages = {6},
	author = {Cowles, Michael and Davis, Caroline},
	date = {1982},
	langid = {english},
	file = {Cowles and Davis - On the Origins of the .05 Level of Statistical Sig.pdf:/home/drosoneuro/Zotero/storage/SAHGIWJZ/Cowles and Davis - On the Origins of the .05 Level of Statistical Sig.pdf:application/pdf}
}

@article{fisher_arrangements_1926,
	title = {The arrangements of field experiments},
	author = {Fisher, Ronald A.},
	date = {1926},
	file = {48.pdf:/home/drosoneuro/Zotero/storage/U5PM8PIA/48.pdf:application/pdf}
}

@misc{neyman_problem_1933,
	title = {On The Problem of the most Efficient Tests of Statistical Hypotheses},
	author = {Neyman, J. and Pearson, E. S.},
	date = {1933},
	file = {rsta.1933.0009.pdf:/home/drosoneuro/Zotero/storage/DYGCNCYD/rsta.1933.0009.pdf:application/pdf}
}

@article{mahoney_publication_1977,
	title = {Publication prejudices: An experimental study of confirmatory bias in the peer review system},
	volume = {1},
	issn = {0147-5916, 1573-2819},
	url = {http://link.springer.com/10.1007/BF01173636},
	doi = {10.1007/BF01173636},
	shorttitle = {Publication prejudices},
	pages = {161--175},
	number = {2},
	journaltitle = {Cognitive Therapy and Research},
	author = {Mahoney, Michael J.},
	urldate = {2019-06-03},
	date = {1977-06},
	langid = {english},
	file = {Mahoney - 1977 - Publication prejudices An experimental study of c.pdf:/home/drosoneuro/Zotero/storage/H726LTCM/Mahoney - 1977 - Publication prejudices An experimental study of c.pdf:application/pdf}
}

@article{flint_there_2015,
	title = {Is there an excess of significant findings in published studies of psychotherapy for depression?},
	volume = {45},
	issn = {0033-2917, 1469-8978},
	url = {https://www.cambridge.org/core/product/identifier/S0033291714001421/type/journal_article},
	doi = {10.1017/S0033291714001421},
	abstract = {Background. Many studies have examined the efﬁcacy of psychotherapy for major depressive disorder ({MDD}) but publication bias against null results may exist in this literature. However, to date, the presence of an excess of signiﬁcant ﬁndings in this literature has not been explicitly tested.
Method. We used a database of 1344 articles on the psychological treatment of depression, identiﬁed through systematic search in {PubMed}, {PsycINFO}, {EMBASE} and the Cochrane database of randomized trials. From these we identiﬁed 149 studies eligible for inclusion that provided 212 comparisons. We tested for an excess of signiﬁcant ﬁndings using the method developed by Ioannidis and Trikalinos (2007), and compared the distribution of p values in this literature with the distribution in the antidepressant literature, where publication bias is known to be operating.
Results. The average statistical power to detect the effect size indicated by the meta-analysis was 49\%. A total of 123 comparisons (58\%) reported a statistically signiﬁcant difference between treatment and control groups, but on the basis of the average power observed, we would only have expected 104 (i.e. 49\%) to do so. There was therefore evidence of an excess of signiﬁcance in this literature (p = 0.010). Similar results were obtained when these analyses were restricted to studies including a cognitive behavioural therapy ({CBT}) arm. Finally, the distribution of p values for psychotherapy studies resembled that for published antidepressant studies, where publication bias against null results has already been established.
Conclusions. The small average size of individual psychotherapy studies is only sufﬁcient to detect large effects. Our results indicate an excess of signiﬁcant ﬁndings relative to what would be expected, given the average statistical power of studies of psychotherapy for major depression.},
	pages = {439--446},
	number = {2},
	journaltitle = {Psychological Medicine},
	author = {Flint, J. and Cuijpers, P. and Horder, J. and Koole, S. L. and Munafò, M. R.},
	urldate = {2019-06-04},
	date = {2015-01},
	langid = {english},
	file = {Flint et al. - 2015 - Is there an excess of significant findings in publ.pdf:/home/drosoneuro/Zotero/storage/3HHAI3GZ/Flint et al. - 2015 - Is there an excess of significant findings in publ.pdf:application/pdf}
}

@article{auspurg_what_2011,
	title = {What Fuels Publication Bias?},
	volume = {231},
	abstract = {Significance tests were originally developed to enable more objective evaluations of research results. Yet the strong orientation towards statistical significance encourages biased results, a phenomenon termed “publication bias”. Publication bias occurs whenever the likelihood or time-lag of publication, or the prominence, language, impact factor of journal space or the citation rate of studies depend on the direction and significance of research findings.},
	pages = {636--660},
	number = {5},
	journaltitle = {Jahrbücher für Nationalökonomie und Statistik / Journal of Economics and Statistics},
	author = {Auspurg, Katrin and Hinz, Thomas},
	date = {2011},
	langid = {english},
	file = {Auspurg and Hinz - What Fuels Publication Bias.pdf:/home/drosoneuro/Zotero/storage/N2DBQTFZ/Auspurg and Hinz - What Fuels Publication Bias.pdf:application/pdf}
}

@article{berning_publication_2016,
	title = {Publication bias in the German social sciences: an application of the caliper test to three top-tier German social science journals},
	volume = {50},
	issn = {0033-5177, 1573-7845},
	url = {http://link.springer.com/10.1007/s11135-015-0182-4},
	doi = {10.1007/s11135-015-0182-4},
	shorttitle = {Publication bias in the German social sciences},
	abstract = {Systematic research reviews have become essential in all empirical sciences. However, the validity of research syntheses is threatened if the preparation, submission or publication of research ﬁndings depends on the statistical signiﬁcance of these ﬁndings. The present study investigates publication bias in three top-tier journals in the German social sciences, utilizing the caliper test. For the period between 2001 and 2010, we have collected 156 articles that appeared in the Ko¨lner Zeitschrift fu¨r Soziologie und Sozialpsychologie ({KZfSS}), the Zeitschrift fu¨r Soziologie ({ZfS}) and the Politische Vierteljahresschrift ({PVS}). In all three journals, we found empirical evidence for the existence of publication bias at the 10 \% signiﬁcance level. We also investigated possible causes linked to this bias, including single versus multiple authorship as well as academic degree. We found only weak support for the relationships between individual author characteristics and publication bias.},
	pages = {901--917},
	number = {2},
	journaltitle = {Quality \& Quantity},
	author = {Berning, Carl C. and Weiß, Bernd},
	urldate = {2019-06-04},
	date = {2016-03},
	langid = {english},
	file = {Berning and Weiß - 2016 - Publication bias in the German social sciences an.pdf:/home/drosoneuro/Zotero/storage/VJDJY54Y/Berning and Weiß - 2016 - Publication bias in the German social sciences an.pdf:application/pdf}
}

@article{weiss_identification_2011,
	title = {The Identification and Prevention of Publication Bias in the Social Sciences and Economics},
	volume = {231},
	url = {http://www.jstor.org/stable/23813343},
	abstract = {Systematic research reviews have become essential in all empirical sciences. However, the va lidity of research syntheses is threatened by the fact that not all studies on a given topic can be summarized. Research reviews may suffer from missing data, and this is especially crucial in those cases where the selectivity of studies and their findings affects the summarized result. So-called publication bias is a type of missing data and a phenomenon that jeopardizes the validity of systematic or quantitative, as well as narrative, reviews. Publication bias exists if the preparation, submission or publication of research findings depend on characteristics of just these research results, e. g. their direction or statistical significance. This article describes methods to identify publication bias in the context of meta-analysis. It also reviews empirical studies on the prevalence of publication bias, especially in the social and economic sciences, where publication bias also seems to be prevalent. Several proposals to prevent publication bias are discussed.},
	pages = {661--684},
	number = {5},
	journaltitle = {Jahrbücher für Nationalökonomie und Statistik / Journal of Economics and Statistics},
	author = {Weiß, Bernd and Wagner, Michael},
	date = {2011},
	langid = {english},
	file = {Weiß and Wagner - 2011 - The Identification and Prevention of Publication B.pdf:/home/drosoneuro/Zotero/storage/EHCWHXTR/Weiß and Wagner - 2011 - The Identification and Prevention of Publication B.pdf:application/pdf}
}

@incollection{dickersin_publication_2005,
	title = {Publication Bias: Recognizing the Problem, Understanding Its Origins and Scope, and Preventing Harm},
	pages = {11--34},
	booktitle = {Publication Bias in Meta-Analysis: Prevention, Assessment and Adjustments},
	publisher = {John Wiley \& Sons Ltd},
	author = {Dickersin, Kay},
	editor = {Rothstein, Hannah R. and Sutton, Alexander J. and Borenstein, Michael},
	date = {2005}
}

@article{kuhberger_publication_2014,
	title = {Publication Bias in Psychology: A Diagnosis Based on the Correlation between Effect Size and Sample Size},
	volume = {9},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0105825},
	doi = {10.1371/journal.pone.0105825},
	shorttitle = {Publication Bias in Psychology},
	abstract = {Background: The p value obtained from a significance test provides no information about the magnitude or importance of the underlying phenomenon. Therefore, additional reporting of effect size is often recommended. Effect sizes are theoretically independent from sample size. Yet this may not hold true empirically: non-independence could indicate publication bias.
Methods: We investigate whether effect size is independent from sample size in psychological research. We randomly sampled 1,000 psychological articles from all areas of psychological research. We extracted p values, effect sizes, and sample sizes of all empirical papers, and calculated the correlation between effect size and sample size, and investigated the distribution of p values.
Results: We found a negative correlation of r = 2.45 [95\% {CI}: 2.53; 2.35] between effect size and sample size. In addition, we found an inordinately high number of p values just passing the boundary of significance. Additional data showed that neither implicit nor explicit power analysis could account for this pattern of findings.
Conclusion: The negative correlation between effect size and samples size, and the biased distribution of p values indicate pervasive publication bias in the entire field of psychology.},
	pages = {e105825},
	number = {9},
	journaltitle = {{PLoS} {ONE}},
	author = {Kühberger, Anton and Fritz, Astrid and Scherndl, Thomas},
	editor = {Fanelli, Daniele},
	urldate = {2019-06-04},
	date = {2014-09-05},
	langid = {english},
	file = {Kühberger et al. - 2014 - Publication Bias in Psychology A Diagnosis Based .PDF:/home/drosoneuro/Zotero/storage/Z2XDGPER/Kühberger et al. - 2014 - Publication Bias in Psychology A Diagnosis Based .PDF:application/pdf}
}

@article{eitan_research_2018,
	title = {Is research in social psychology politically biased? Systematic empirical tests and a forecasting survey to address the controversy},
	volume = {79},
	issn = {00221031},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022103118300416},
	doi = {10.1016/j.jesp.2018.06.004},
	shorttitle = {Is research in social psychology politically biased?},
	abstract = {The present investigation provides the ﬁrst systematic empirical tests for the role of politics in academic research. In a large sample of scientiﬁc abstracts from the ﬁeld of social psychology, we ﬁnd both evaluative diﬀerences, such that conservatives are described more negatively than liberals, and explanatory diﬀerences, such that conservatism is more likely to be the focus of explanation than liberalism. In light of the ongoing debate about politicized science, a forecasting survey permitted scientists to state a priori empirical predictions about the results, and then change their beliefs in light of the evidence. Participating scientists accurately predicted the direction of both the evaluative and explanatory diﬀerences, but at the same time signiﬁcantly overestimated both eﬀect sizes. Scientists also updated their broader beliefs about political bias in response to the empirical results, providing a model for addressing divisive scientiﬁc controversies across ﬁelds.},
	pages = {188--199},
	journaltitle = {Journal of Experimental Social Psychology},
	author = {Eitan, Orly and Viganola, Domenico and Inbar, Yoel and Dreber, Anna and Johannesson, Magnus and Pfeiffer, Thomas and Thau, Stefan and Uhlmann, Eric Luis},
	urldate = {2019-06-04},
	date = {2018-11},
	langid = {english},
	file = {Eitan et al. - 2018 - Is research in social psychology politically biase.pdf:/home/drosoneuro/Zotero/storage/G8KPB2J8/Eitan et al. - 2018 - Is research in social psychology politically biase.pdf:application/pdf}
}

@article{gerber_testing_2001,
	title = {Testing for Publication Bias in Political Science},
	volume = {9},
	url = {http://www.jstor.org/stable/25791658},
	pages = {385--392},
	number = {4},
	journaltitle = {Political Analysis},
	author = {Gerber, Alan S. and Green, Donald P. and Nickerson, David},
	date = {2001},
	langid = {english},
	file = {Gerber et al. - 2001 - Testing for Publication Bias in Political Science.pdf:/home/drosoneuro/Zotero/storage/3GJ7W893/Gerber et al. - 2001 - Testing for Publication Bias in Political Science.pdf:application/pdf}
}

@article{begg_publication_1988,
	title = {Publication Bias: A Problem in Interpreting Medical Data},
	volume = {151},
	issn = {09641998},
	url = {https://www.jstor.org/stable/10.2307/2982993?origin=crossref},
	doi = {10.2307/2982993},
	shorttitle = {Publication Bias},
	abstract = {Publication bias, the phenomenon in which studies with positive results are more likely to be published than studies with negative results, is a serious problem in the interpretation of scientificresearch.Various hypothetical models have been studied which clarifythe potential for bias and highlight characteristics which make a study especially susceptible to bias. Empirical investigations have supported the hypothesis that bias exists and have provided a quantitative assessment of the magnitude of the problem. The use of meta-analysis as a research tool has focused attention on the issue, since naive methodologies in this area are especially susceptible to bias. In this paper we review the available research, discuss alternative suggestions for conducting unbiased meta-analysis and suggest some scientific policy measures which could improve the quality of published data in the long term.},
	pages = {419},
	number = {3},
	journaltitle = {Journal of the Royal Statistical Society. Series A (Statistics in Society)},
	author = {Begg, Colin B. and Berlin, Jesse A.},
	urldate = {2019-06-05},
	date = {1988},
	langid = {english},
	file = {Begg and Berlin - 1988 - Publication Bias A Problem in Interpreting Medica.pdf:/home/drosoneuro/Zotero/storage/MPKVXUZQ/Begg and Berlin - 1988 - Publication Bias A Problem in Interpreting Medica.pdf:application/pdf}
}

@article{pashler_replicability_2012,
	title = {Is the Replicability Crisis Overblown? Three Arguments Examined},
	volume = {7},
	issn = {1745-6916, 1745-6924},
	url = {http://journals.sagepub.com/doi/10.1177/1745691612463401},
	doi = {10.1177/1745691612463401},
	shorttitle = {Is the Replicability Crisis Overblown?},
	abstract = {We discuss three arguments voiced by scientists who view the current outpouring of concern about replicability as overblown. The first idea is that the adoption of a low alpha level (e.g., 5\%) puts reasonable bounds on the rate at which errors can enter the published literature, making false-positive effects rare enough to be considered a minor issue. This, we point out, rests on statistical misunderstanding: The alpha level imposes no limit on the rate at which errors may arise in the literature (Ioannidis, 2005b). Second, some argue that whereas direct replication attempts are uncommon, conceptual replication attempts are common—providing an even better test of the validity of a phenomenon. We contend that performing conceptual rather than direct replication attempts interacts insidiously with publication bias, opening the door to literatures that appear to confirm the reality of phenomena that in fact do not exist. Finally, we discuss the argument that errors will eventually be pruned out of the literature if the field would just show a bit of patience. We contend that there are no plausible concrete scenarios to back up such forecasts and that what is needed is not patience, but rather systematic reforms in scientific practice.},
	pages = {531--536},
	number = {6},
	journaltitle = {Perspectives on Psychological Science},
	author = {Pashler, Harold and Harris, Christine R.},
	urldate = {2019-06-06},
	date = {2012-11},
	langid = {english},
	file = {Pashler and Harris - 2012 - Is the Replicability Crisis Overblown Three Argum.pdf:/home/drosoneuro/Zotero/storage/CXEQT3UI/Pashler and Harris - 2012 - Is the Replicability Crisis Overblown Three Argum.pdf:application/pdf}
}

@article{kerr_harking_1998,
	title = {{HARKing}: Hypothesizing After the Results are Known},
	volume = {2},
	issn = {1088-8683, 1532-7957},
	url = {http://journals.sagepub.com/doi/10.1207/s15327957pspr0203_4},
	doi = {10.1207/s15327957pspr0203_4},
	shorttitle = {{HARKing}},
	abstract = {My purpose here is not to propose a definition for {HARKing} that can be unambiguously applied in any particular instance. Nor do I suggest (or believe) that the various forms {HARKing} might take are equivalent (e.g., in terms of their costs). My immediate purpose is to define conceptually and illustrate a (fuzzy) set of approaches to advancing hypotheses in research reports that share a common feature-namely, knowledge of the results of the study alters the set of hypotheses advanced in the report's introduction.},
	pages = {196--217},
	number = {3},
	journaltitle = {Personality and Social Psychology Review},
	author = {Kerr, Norbert L.},
	urldate = {2019-06-06},
	date = {1998-08},
	langid = {english},
	file = {Kerr - 1998 - HARKing Hypothesizing After the Results are Known.pdf:/home/drosoneuro/Zotero/storage/NIUJ8P63/Kerr - 1998 - HARKing Hypothesizing After the Results are Known.pdf:application/pdf}
}

@article{head_extent_2015,
	title = {The Extent and Consequences of P-Hacking in Science},
	volume = {13},
	issn = {1545-7885},
	url = {https://dx.plos.org/10.1371/journal.pbio.1002106},
	doi = {10.1371/journal.pbio.1002106},
	abstract = {A focus on novel, confirmatory, and statistically significant results leads to substantial bias in the scientific literature. One type of bias, known as “p-hacking,” occurs when researchers collect or select data or statistical analyses until nonsignificant results become significant. Here, we use text-mining to demonstrate that p-hacking is widespread throughout science. We then illustrate how one can test for p-hacking when performing a meta-analysis and show that, while p-hacking is probably common, its effect seems to be weak relative to the real effect sizes being measured. This result suggests that p-hacking probably does not drastically alter scientific consensuses drawn from meta-analyses.},
	pages = {e1002106},
	number = {3},
	journaltitle = {{PLOS} Biology},
	author = {Head, Megan L. and Holman, Luke and Lanfear, Rob and Kahn, Andrew T. and Jennions, Michael D.},
	urldate = {2019-06-06},
	date = {2015-03-13},
	langid = {english},
	file = {Head et al. - 2015 - The Extent and Consequences of P-Hacking in Scienc.pdf:/home/drosoneuro/Zotero/storage/HRHAHW5N/Head et al. - 2015 - The Extent and Consequences of P-Hacking in Scienc.pdf:application/pdf}
}

@article{simmons_falsepositive_2011,
	title = {False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant},
	volume = {22},
	issn = {0956-7976, 1467-9280},
	url = {http://journals.sagepub.com/doi/10.1177/0956797611417632},
	doi = {10.1177/0956797611417632},
	shorttitle = {False-Positive Psychology},
	abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists’ nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
	pages = {1359--1366},
	number = {11},
	journaltitle = {Psychological Science},
	author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
	urldate = {2019-06-06},
	date = {2011-11},
	langid = {english},
	file = {Simmons et al. - 2011 - False-Positive Psychology Undisclosed Flexibility.pdf:/home/drosoneuro/Zotero/storage/2EBNFVE6/Simmons et al. - 2011 - False-Positive Psychology Undisclosed Flexibility.pdf:application/pdf}
}

@article{chapman_innovative_2013,
	title = {Innovative estimation of survival using log-normal survival modelling on {ACCENT} database},
	volume = {108},
	issn = {0007-0920, 1532-1827},
	url = {http://www.nature.com/articles/bjc201334},
	doi = {10.1038/bjc.2013.34},
	abstract = {Background: The {ACCENT} database, with individual patient data for 20 898 patients from 18 colon cancer clinical trials, was used to support Food and Drug Administration ({FDA}) approval of 3-year disease-free survival as a surrogate for 5-year overall survival. We hypothesised substantive differences in survival estimation with log-normal modelling rather than standard Kaplan–Meier or Cox approaches.
Methods: Time to relapse, disease-free survival, and overall survival were estimated using Kaplan–Meier, Cox, and log-normal approaches for male subjects aged 60–65 years, with stage {III} colon cancer, treated with 5-fluorouracil-based chemotherapy regimens (with 5FU), or with surgery alone (without 5FU).
Results: Absolute differences between Cox and log-normal estimates with (without) 5FU varied by end point. The log-normal model had 5.8 (6.3)\% higher estimated 3-year time to relapse than the Cox model; 4.8 (5.1)\% higher 3-year disease-free survival; and 3.2 (2.2)\% higher 5-year overall survival. Model checking indicated greater data support for the log-normal than the Cox model, with Cox and Kaplan–Meier estimates being more similar. All three model types indicate consistent evidence of treatment benefit on both 3-year disease-free survival and 5-year overall survival; patients allocated to 5FU had 5.0–6.7\% higher 3-year disease-free survival and 5.3–6.8\% higher 5-year overall survival.
Conclusion: Substantive absolute differences between estimates of 3-year disease-free survival and 5-year overall survival with lognormal and Cox models were large enough to be clinically relevant, and warrant further consideration.},
	pages = {784--790},
	number = {4},
	journaltitle = {British Journal of Cancer},
	author = {Chapman, J W and O'Callaghan, C J and Hu, N and Ding, K and Yothers, G A and Catalano, P J and Shi, Q and Gray, R G and O'Connell, M J and Sargent, D J},
	urldate = {2019-06-07},
	date = {2013-03},
	langid = {english},
	file = {for the ACCENT collaborative group et al. - 2013 - Innovative estimation of survival using log-normal.pdf:/home/drosoneuro/Zotero/storage/H4IT37SY/for the ACCENT collaborative group et al. - 2013 - Innovative estimation of survival using log-normal.pdf:application/pdf}
}

@article{royston_lognormal_2001,
	title = {The Lognormal Distribution as a Model for Survival Time in Cancer, With an Emphasis on Prognostic Factors},
	volume = {55},
	issn = {0039-0402, 1467-9574},
	url = {http://doi.wiley.com/10.1111/1467-9574.00158},
	doi = {10.1111/1467-9574.00158},
	abstract = {Despite their long history, parametric survival-time models have largely been neglected in the modern biostatistical and medical literature in favour of the Cox proportional hazards model. Here, I present a case for the use of the lognormal distribution in the analysis of survival times of breast and ovarian cancer patients, speci®cally in modelling the effects of prognostic factors. The lognormal provides a completely speci®ed probability distribution for the observations and a sensible estimate of the variation explained by the model, a quantity that is controversial for the Cox model. I show how imputation of censored observations under the model may be used to inspect the data using familiar graphical and other technques. Results from the Cox and lognormal models are compared and shown apparently to differ to some extent. However, it is hard to judge which model gives the more accurate estimates. It is concluded that provided the lognormal model ®ts the data adequately, it may be a useful approach to the analysis of censored survival data.},
	pages = {89--104},
	number = {1},
	journaltitle = {Statistica Neerlandica},
	author = {Royston, P.},
	urldate = {2019-06-07},
	date = {2001-03},
	langid = {english},
	file = {Royston - 2001 - The Lognormal Distribution as a Model for Survival.pdf:/home/drosoneuro/Zotero/storage/NTG4IKF3/Royston - 2001 - The Lognormal Distribution as a Model for Survival.pdf:application/pdf}
}

@article{angus_probability_1994,
	title = {The Probability Integral Transform and Related Results},
	volume = {36},
	issn = {0036-1445, 1095-7200},
	url = {http://epubs.siam.org/doi/10.1137/1036146},
	doi = {10.1137/1036146},
	abstract = {A simple proof of the probability integral transform theorem in probability and statistics is given that depends only on probabilistic concepts and elementary properties of continuous functions. This proof yields the theorem in its fullest generality. A similar theorem that forms the basis for the inverse method of random number generation is also discussed and contrasted to the probability integral transform theorem. Typical applications are discussed. Despite their generality and far reaching consequences, these theorems are remarkable in their simplicity and ease of proof.},
	pages = {652--654},
	number = {4},
	journaltitle = {{SIAM} Review},
	author = {Angus, John E.},
	urldate = {2019-06-07},
	date = {1994-12},
	langid = {english},
	file = {Angus - 1994 - The Probability Integral Transform and Related Res.pdf:/home/drosoneuro/Zotero/storage/WXG8U5Z6/Angus - 1994 - The Probability Integral Transform and Related Res.pdf:application/pdf}
}

@book{casella_statistical_2002,
	location = {Australia ; Pacific Grove, {CA}},
	edition = {2nd ed},
	title = {Statistical inference},
	isbn = {978-0-534-24312-8},
	pagetotal = {660},
	publisher = {Thomson Learning},
	author = {Casella, George and Berger, Roger L.},
	date = {2002},
	langid = {english},
	keywords = {Mathematical statistics, Probabilities}
}

@article{murdoch_values_2008,
	title = {\textit{P} -Values are Random Variables},
	volume = {62},
	issn = {0003-1305, 1537-2731},
	url = {http://www.tandfonline.com/doi/abs/10.1198/000313008X332421},
	doi = {10.1198/000313008X332421},
	pages = {242--245},
	number = {3},
	journaltitle = {The American Statistician},
	author = {Murdoch, Duncan J and Tsai, Yu-Ling and Adcock, James},
	urldate = {2019-06-07},
	date = {2008-08},
	langid = {english},
	file = {Murdoch et al. - 2008 - iPi -Values are Random Variables.pdf:/home/drosoneuro/Zotero/storage/KY8BKRKQ/Murdoch et al. - 2008 - iPi -Values are Random Variables.pdf:application/pdf}
}

@article{sterne_funnel_2001,
	title = {Funnel plots for detecting bias in meta-analysis: Guidelines on choice of axis},
	abstract = {Asymmetry in funnel plots may indicate publication bias in meta-analysis, but the shape of the plot in the absence of bias depends on the choice of axes. We evaluated standard error, precision (inverse of standard error), variance, inverse of variance, sample size and log sample size (vertical axis) and log odds ratio, log risk ratio and risk difference (horizontal axis). Standard error is likely to be the best choice for the vertical axis: the expected shape in the absence of bias corresponds to a symmetrical funnel, straight lines to indicate 95\% confidence intervals can be included and the plot emphasises smaller studies which are more prone to bias. Precision or inverse of variance is useful when comparing meta-analyses of small trials with subsequent large trials. The use of sample size or log sample size is problematic because the expected shape of the plot in the absence of bias is unpredictable. We found similar evidence for asymmetry and between trial variation in a sample of 78 published meta-analyses whether odds ratios or risk ratios were used on the horizontal axis. Different conclusions were reached for risk differences and this was related to increased between-trial variation. We conclude that funnel plots of meta-analyses should generally use standard error as the measure of study size and ratio measures of treatment effect. © 2001 Elsevier Science Inc. All rights reserved.},
	pages = {10},
	journaltitle = {Journal of Clinical Epidemiology},
	author = {Sterne, Jonathan A C and Egger, Matthias},
	date = {2001},
	langid = {english},
	file = {Sterne and Egger - 2001 - Funnel plots for detecting bias in meta-analysis .pdf:/home/drosoneuro/Zotero/storage/I6N8V577/Sterne and Egger - 2001 - Funnel plots for detecting bias in meta-analysis .pdf:application/pdf}
}

@article{terrin_empirical_2005,
	title = {In an empirical evaluation of the funnel plot, researchers could not visually identify publication bias},
	volume = {58},
	issn = {08954356},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S089543560500082X},
	doi = {10.1016/j.jclinepi.2005.01.006},
	abstract = {Background and Objective: Publication bias and related biases can lead to overly optimistic conclusions in systematic reviews. The funnel plot, which is frequently used to detect such biases, has not yet been subjected to empirical evaluation as a visual tool. We sought to determine whether researchers can correctly identify publication bias from visual inspection of funnel plots in typical-size systematic reviews.
Methods: A questionnaire with funnel plots containing 10 studies each (the median number in medical meta-analyses) was completed by 41 medical researchers, including clinical research fellows in a meta-analysis class, faculty in clinical care research, and experienced systematic reviewers.
Results: On average, participants correctly identiﬁed 52.5\% (95\% {CI} 50.6–54.4\%) of the plots as being affected or unaffected by publication bias. The weighted mean percent correct, which adjusted for the fact that asymmetric plots are more likely to occur in the presence of publication bias, was also low (48.3 to 62.8\%, depending on the presence or absence of publication bias and heterogeneous study effects).
Conclusion: Researchers who assess for publication bias using the funnel plot may be misled by its shape. Authors and readers of systematic reviews need to be aware of the limitations of the funnel plot. Ć 2005 Elsevier Inc. All rights reserved.},
	pages = {894--901},
	number = {9},
	journaltitle = {Journal of Clinical Epidemiology},
	author = {Terrin, Norma and Schmid, Christopher H. and Lau, Joseph},
	urldate = {2019-06-09},
	date = {2005-09},
	langid = {english},
	file = {Terrin et al. - 2005 - In an empirical evaluation of the funnel plot, res.pdf:/home/drosoneuro/Zotero/storage/3PAL86ZI/Terrin et al. - 2005 - In an empirical evaluation of the funnel plot, res.pdf:application/pdf}
}

@article{tang_misleading_2000,
	title = {Misleading funnel plot for detection of bias in meta-analysis},
	volume = {53},
	issn = {08954356},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0895435699002048},
	doi = {10.1016/S0895-4356(99)00204-8},
	abstract = {Publication and other forms of selection biases pose a threat to the validity of meta-analysis. Funnel plots are usually used to detect such biases; asymmetrical plots are interpreted to suggest that biases are present. Using 198 published meta-analyses, we demonstrate that the shape of a funnel plot is largely determined by the arbitrary choice of the method to construct the plot. When a different definition of precision and/or effect measure were used, the conclusion about the shape of the plot was altered in 37 (86\%) of the 43 meta-analyses with an asymmetrical plot suggesting selection bias. In the absence of a consensus on how the plot should be constructed, asymmetrical funnel plots should be interpreted cautiously. These findings also suggest that the discrepancies between large trials and corresponding meta-analyses and heterogeneity in meta-analyses may also be determined by how they are evaluated. © 2000 Elsevier Science Inc. All rights reserved.},
	pages = {477--484},
	number = {5},
	journaltitle = {Journal of Clinical Epidemiology},
	author = {Tang, Jin-Ling and Liu, Joseph {LY}},
	urldate = {2019-06-09},
	date = {2000-05},
	langid = {english},
	file = {Tang and Liu - 2000 - Misleading funnel plot for detection of bias in me.pdf:/home/drosoneuro/Zotero/storage/ZVMXSEJI/Tang and Liu - 2000 - Misleading funnel plot for detection of bias in me.pdf:application/pdf}
}

@book{light_summing_1984,
	title = {Summing up: The Science of Reviewing Research},
	publisher = {Harvard University Press},
	author = {Light, Richard J. and Pillemer, David B.},
	date = {1984}
}

@incollection{sterne_funnel_2005,
	title = {The Funnel Plot},
	pages = {75--98},
	booktitle = {Publication Bias in Meta-Analysis: Prevention, Assessment and Adjustments},
	publisher = {John Wiley \& Sons Ltd},
	author = {Sterne, Jonathan A C and Becker, Betsy Jane and Egger, Matthias},
	editor = {Rothstein, Hannah R. and Sutton, Alexander J. and Borenstein, Michael},
	date = {2005}
}

@incollection{sterne_regression_2005,
	title = {Regression Methods to Detect Publication and Other Bias in Meta-Analysis},
	pages = {99--110},
	booktitle = {Publication Bias in Meta-Analysis: Prevention, Assessment and Adjustments},
	publisher = {John Wiley \& Sons Ltd},
	author = {Sterne, Jonathan A C and Egger, Matthias},
	editor = {Rothstein, Hannah R. and Sutton, Alexander J. and Borenstein, Michael},
	date = {2005}
}

@article{peters_comparison_2006,
	title = {Comparison of Two Methods to Detect Publication Bias in Meta-analysis},
	volume = {295},
	issn = {0098-7484},
	url = {http://jama.jamanetwork.com/article.aspx?doi=10.1001/jama.295.6.676},
	doi = {10.1001/jama.295.6.676},
	abstract = {Objective To compare the performance of Egger’s regression test with a regression test based on sample size (a modification of Macaskill’s test) with {lnOR} as the summary estimate. Design Simulation of meta-analyses under a number of scenarios in the presence and absence of publication bias and between-study heterogeneity. Main Outcome Measures Type I error rates (the proportion of false-positive results) for each regression test and their power to detect publication bias when it is present (the proportion of true-positive results).
Results Type I error rates for Egger’s regression test are higher than those for the alternative regression test. The alternative regression test has the appropriate type I error rates regardless of the size of the underlying {OR}, the number of primary studies in the meta-analysis, and the level of between-study heterogeneity. The alternative regression test has comparable power to Egger’s regression test to detect publication bias under conditions of low between-study heterogeneity.
Conclusion Because of appropriate type I error rates and reduction in the correlation between the {lnOR} and its variance, the alternative regression test can be used in place of Egger’s regression test when the summary estimates are {lnORs}.},
	pages = {676},
	number = {6},
	journaltitle = {{JAMA}},
	author = {Peters, Jaime L.},
	urldate = {2019-06-09},
	date = {2006-02-08},
	langid = {english},
	file = {Peters - 2006 - Comparison of Two Methods to Detect Publication Bi.pdf:/home/drosoneuro/Zotero/storage/ZV9MVTWR/Peters - 2006 - Comparison of Two Methods to Detect Publication Bi.pdf:application/pdf}
}

@article{moreno_assessment_2009,
	title = {Assessment of regression-based methods to adjust for publication bias through a comprehensive simulation study},
	volume = {9},
	issn = {1471-2288},
	url = {https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-9-2},
	doi = {10.1186/1471-2288-9-2},
	abstract = {Background: In meta-analysis, the presence of funnel plot asymmetry is attributed to publication or other small-study effects, which causes larger effects to be observed in the smaller studies. This issue potentially mean inappropriate conclusions are drawn from a meta-analysis. If meta-analysis is to be used to inform decision-making, a reliable way to adjust pooled estimates for potential funnel plot asymmetry is required.
Methods: A comprehensive simulation study is presented to assess the performance of different adjustment methods including the novel application of several regression-based methods (which are commonly applied to detect publication bias rather than adjust for it) and the popular Trim \& Fill algorithm. Meta-analyses with binary outcomes, analysed on the log odds ratio scale, were simulated by considering scenarios with and without i) publication bias and; ii) heterogeneity. Publication bias was induced through two underlying mechanisms assuming the probability of publication depends on i) the study effect size; or ii) the p-value.
Results: The performance of all methods tended to worsen as unexplained heterogeneity increased and the number of studies in the meta-analysis decreased. Applying the methods conditional on an initial test for the presence of funnel plot asymmetry generally provided poorer performance than the unconditional use of the adjustment method. Several of the regression based methods consistently outperformed the Trim \& Fill estimators.
Conclusion: Regression-based adjustments for publication bias and other small study effects are easy to conduct and outperformed more established methods over a wide range of simulation scenarios.},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	author = {Moreno, Santiago G and Sutton, Alex J and Ades, Ae and Stanley, Tom D and Abrams, Keith R and Peters, Jaime L and Cooper, Nicola J},
	urldate = {2019-06-09},
	date = {2009-12},
	langid = {english},
	file = {Moreno et al. - 2009 - Assessment of regression-based methods to adjust f.pdf:/home/drosoneuro/Zotero/storage/PRLQSEE5/Moreno et al. - 2009 - Assessment of regression-based methods to adjust f.pdf:application/pdf}
}

@article{macaskill_comparison_2001,
	title = {A comparison of methods to detect publication bias in meta-analysis},
	volume = {20},
	issn = {0277-6715, 1097-0258},
	url = {http://doi.wiley.com/10.1002/sim.698},
	doi = {10.1002/sim.698},
	abstract = {Meta-analyses are subject to bias for many of reasons, including publication bias. Asymmetry in a funnel plot of study size against treatment e ect is often used to identify such bias. We compare the performance of three simple methods of testing for bias: the rank correlation method; a simple linear regression of the standardized estimate of treatment e ect on the precision of the estimate; and a regression of the treatment e ect on sample size. The tests are applied to simulated meta-analyses in the presence and absence of publication bias. Both one-sided and two-sided censoring of studies based on statistical signiÿcance was used. The results indicate that none of the tests performs consistently well. Test performance varied with the magnitude of the true treatment e ect, distribution of study size and whether a one- or two-tailed signiÿcance test was employed. Overall, the power of the tests was low when the number of studies per meta-analysis was close to that often observed in practice. Tests that showed the highest power also had type I error rates higher than the nominal level. Based on the empirical type I error rates, a regression of treatment e ect on sample size, weighted by the inverse of the variance of the logit of the pooled proportion (using the marginal total) is the preferred method. Copyright ? 2001 John Wiley \& Sons, Ltd.},
	pages = {641--654},
	number = {4},
	journaltitle = {Statistics in Medicine},
	author = {Macaskill, Petra and Walter, Stephen D. and Irwig, Les},
	urldate = {2019-06-09},
	date = {2001-02-28},
	langid = {english},
	file = {Macaskill et al. - 2001 - A comparison of methods to detect publication bias.pdf:/home/drosoneuro/Zotero/storage/G2Q5PHGF/Macaskill et al. - 2001 - A comparison of methods to detect publication bias.pdf:application/pdf}
}

@article{kendall_new_1938,
	title = {A New Measure of Rank Correlation},
	volume = {30},
	pages = {81--93},
	number = {1},
	journaltitle = {Biometrika},
	author = {Kendall, M G},
	date = {1938},
	langid = {english},
	file = {Kendall - A New Measure of Rank Correlation.pdf:/home/drosoneuro/Zotero/storage/5L7A5D6T/Kendall - A New Measure of Rank Correlation.pdf:application/pdf}
}

@article{duval_nonparametric_2000,
	title = {A Nonparametric “Trim and Fill” Method of Accounting for Publication Bias in Meta-Analysis},
	volume = {95},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.2000.10473905},
	doi = {10.1080/01621459.2000.10473905},
	pages = {89--98},
	number = {449},
	journaltitle = {Journal of the American Statistical Association},
	author = {Duval, Sue and Tweedie, Richard},
	urldate = {2019-06-10},
	date = {2000-03},
	langid = {english},
	file = {Duval and Tweedie - 2000 - A Nonparametric “Trim and Fill” Method of Accounti.pdf:/home/drosoneuro/Zotero/storage/B95PSF2D/Duval and Tweedie - 2000 - A Nonparametric “Trim and Fill” Method of Accounti.pdf:application/pdf}
}

@incollection{duval_trim_2005,
	title = {The Trim and Fill Method},
	pages = {127--144},
	booktitle = {Publication Bias in Meta-Analysis: Prevention, Assessment and Adjustments},
	publisher = {John Wiley \& Sons Ltd},
	author = {Duval, Sue},
	editor = {Rothstein, Hannah R. and Sutton, Alexander J. and Borenstein, Michael},
	date = {2005}
}

@article{fanelli_negative_2012,
	title = {Negative results are disappearing from most disciplines and countries},
	volume = {90},
	issn = {0138-9130, 1588-2861},
	url = {http://link.springer.com/10.1007/s11192-011-0494-7},
	doi = {10.1007/s11192-011-0494-7},
	abstract = {Concerns that the growing competition for funding and citations might distort science are frequently discussed, but have not been veriﬁed directly. Of the hypothesized problems, perhaps the most worrying is a worsening of positive-outcome bias. A system that disfavours negative results not only distorts the scientiﬁc literature directly, but might also discourage high-risk projects and pressure scientists to fabricate and falsify their data. This study analysed over 4,600 papers published in all disciplines between 1990 and 2007, measuring the frequency of papers that, having declared to have ‘‘tested’’ a hypothesis, reported a positive support for it. The overall frequency of positive supports has grown by over 22\% between 1990 and 2007, with signiﬁcant differences between disciplines and countries. The increase was stronger in the social and some biomedical disciplines. The United States had published, over the years, signiﬁcantly fewer positive results than Asian countries (and particularly Japan) but more than European countries (and in particular the United Kingdom). Methodological artefacts cannot explain away these patterns, which support the hypotheses that research is becoming less pioneering and/or that the objectivity with which results are produced and published is decreasing.},
	pages = {891--904},
	number = {3},
	journaltitle = {Scientometrics},
	author = {Fanelli, Daniele},
	urldate = {2019-06-11},
	date = {2012-03},
	langid = {english},
	file = {Fanelli - 2012 - Negative results are disappearing from most discip.pdf:/home/drosoneuro/Zotero/storage/NEMMBRVN/Fanelli - 2012 - Negative results are disappearing from most discip.pdf:application/pdf}
}

@book{barlow_statistics_1989,
	title = {Statistics: a guide to the use of statistical methods in the physical sciences},
	volume = {29},
	publisher = {John Wiley \& Sons},
	author = {Barlow, Roger J},
	date = {1989}
}

@article{stanley_metaregression_2005,
	title = {Meta-Regression Analysis: A Quantitative Method of Literature Surveys: Meta-Regression Analysis},
	volume = {19},
	issn = {09500804},
	url = {http://doi.wiley.com/10.1111/j.0950-0804.2005.00249.x},
	doi = {10.1111/j.0950-0804.2005.00249.x},
	shorttitle = {Meta-Regression Analysis},
	abstract = {Pedagogically, literature reviews are instrumental. They summarize the large literature written on a particular topic, give coherence to the complex, often disparate, views expressed about an issue, and serve as a springboard for new ideas. However, literature surveys rarely establish anything approximating unanimous consensus. Ironically, this is just as true for the empirical economic literature. To harmonize this dissonance, we offer a quantitative methodology for reviewing the empirical economic literature. Meta-regression analysis ({MRA}) is the regression analysis of regression analyses. {MRA} tends to objectify the review process. It studies the processes that produce empirical economic results as though they were any other social scientific phenomenon. {MRA} provides a framework for replication and offers a sensitivity analysis for model specification. In this brief essay, we propose a new method of reviewing economic literature, {MRA}, and discuss its potential.},
	pages = {299--308},
	number = {3},
	journaltitle = {Journal of Economic Surveys},
	author = {Stanley, T. D. and Jarrell, Stephen B.},
	urldate = {2019-06-11},
	date = {2005-07},
	langid = {english},
	file = {Stanley and Jarrell - 2005 - Meta-Regression Analysis A Quantitative Method of.pdf:/home/drosoneuro/Zotero/storage/E9GIILBQ/Stanley and Jarrell - 2005 - Meta-Regression Analysis A Quantitative Method of.pdf:application/pdf}
}

@book{fisherbox_fisher_1978,
	title = {R. A. Fisher, The Life of a Scientist},
	publisher = {John Wiley \& Sons},
	author = {Fisher Box, Joan},
	date = {1978}
}

@article{fisher_presidential_1938,
	title = {Presidential address to the first Indian statistical congress},
	volume = {4},
	pages = {14--17},
	number = {14},
	journaltitle = {Sankhya},
	author = {Fisher, Ronald A},
	date = {1938},
	file = {Fisher - Presidential Address.pdf:/home/drosoneuro/Zotero/storage/QHYCER9X/Fisher - Presidential Address.pdf:application/pdf}
}

@book{kulinskaya_meta_2008,
	location = {Chichester},
	title = {Meta analysis: a guide to calibrating and combining statistical evidence},
	isbn = {978-0-470-02864-3},
	series = {Wiley series in probability and statistics},
	shorttitle = {Meta analysis},
	pagetotal = {260},
	publisher = {Wiley},
	author = {Kulinskaya, Elena and Morgenthaler, Stephan and Staudte, Robert G.},
	date = {2008},
	langid = {english},
	note = {{OCLC}: 603590364},
	file = {Kulinskaya et al. - 2008 - Meta analysis a guide to calibrating and combinin.pdf:/home/drosoneuro/Zotero/storage/ECQ78SH7/Kulinskaya et al. - 2008 - Meta analysis a guide to calibrating and combinin.pdf:application/pdf}
}

@book{johnson_univariate_2005,
	title = {Univariate discrete distributions},
	volume = {444},
	publisher = {John Wiley \& Sons},
	author = {Johnson, Norman L and Kemp, Adrienne W and Kotz, Samuel},
	date = {2005}
}

@article{welch_generalization_1947,
	title = {The Generalization of `Student's' Problem when Several Different Population Variances are Involved},
	volume = {34},
	issn = {00063444},
	url = {https://www.jstor.org/stable/2332510?origin=crossref},
	doi = {10.2307/2332510},
	pages = {28},
	number = {1},
	journaltitle = {Biometrika},
	author = {Welch, B. L.},
	urldate = {2019-06-14},
	date = {1947-01},
	langid = {english},
	file = {Welch - 1947 - The Generalization of `Student's' Problem when Sev.pdf:/home/drosoneuro/Zotero/storage/TM72NMEC/Welch - 1947 - The Generalization of `Student's' Problem when Sev.pdf:application/pdf}
}

@inproceedings{gerber_can_2006,
	title = {Can political science literatures be believed? A study of publication bias in the {APSR} and the {AJPS}},
	booktitle = {Annual Meeting of the Midwest Political Science Association},
	publisher = {{CiteseerX}},
	author = {Gerber, Alan and Malhotra, Neil},
	date = {2006},
	file = {Gerber and Malhotra - Can political science literatures be believed.pdf:/home/drosoneuro/Zotero/storage/CVDDYUMW/Gerber and Malhotra - Can political science literatures be believed.pdf:application/pdf}
}

@article{simonsohn_pcurve_detection_2014,
	title = {P-curve: a key to the file-drawer.},
	volume = {143},
	pages = {534},
	number = {2},
	journaltitle = {Journal of Experimental Psychology: General},
	author = {Simonsohn, Uri and Nelson, Leif D and Simmons, Joseph P},
	date = {2014},
	note = {bibtex: simonsohn\_pcurve\_detection\_2014},
	file = {Simonsohn et al. - P-Curve A Key to the File-Drawer.pdf:/home/drosoneuro/Zotero/storage/EFMA777M/Simonsohn et al. - P-Curve A Key to the File-Drawer.pdf:application/pdf;Supplement_pcurve1.pdf:/home/drosoneuro/Zotero/storage/8IL4Z9VN/Supplement_pcurve1.pdf:application/pdf}
}

@article{simonsohn_pcurve_correction_2014,
	title = {P-curve and effect size: Correcting for publication bias using only significant results},
	volume = {9},
	pages = {666--681},
	number = {6},
	journaltitle = {Perspectives on Psychological Science},
	author = {Simonsohn, Uri and Nelson, Leif D and Simmons, Joseph P},
	date = {2014},
	note = {bibtex: simonsohn\_pcurve\_correction\_2014},
	file = {Simonsohn et al. - 2014 - ipi -Curve and Effect Size Correcting for Pu.pdf:/home/drosoneuro/Zotero/storage/X53UZG29/Simonsohn et al. - 2014 - ipi -Curve and Effect Size Correcting for Pu.pdf:application/pdf}
}

@article{hansen_theory_1943,
	title = {On the Theory of Sampling from Finite Populations},
	volume = {14},
	issn = {0003-4851},
	url = {http://projecteuclid.org/euclid.aoms/1177731356},
	doi = {10.1214/aoms/1177731356},
	pages = {333--362},
	number = {4},
	journaltitle = {The Annals of Mathematical Statistics},
	author = {Hansen, Morris H. and Hurwitz, William N.},
	urldate = {2019-06-20},
	date = {1943-12},
	langid = {english},
	file = {Hansen and Hurwitz - 1943 - On the Theory of Sampling from Finite Populations.pdf:/home/drosoneuro/Zotero/storage/3HCT9N73/Hansen and Hurwitz - 1943 - On the Theory of Sampling from Finite Populations.pdf:application/pdf}
}

@article{copas_what_1999,
	title = {What Works?: Selectivity Models and Meta-Analysis},
	volume = {162},
	url = {http://www.jstor.org/stable/2680469},
	abstract = {Whatworksseeks to identifyrehabilitativtereatmentswhichare successful in reducing the {likelihoodthatoffenderswillreoffendA}. large numberofsmall case-controlstudies have been reportedintheliteratureb,utwithconflictinrgesults.Meta-analysishas been used to reconcilethese findingsb,utagain withconflictinrgesults.We reanalyseone ofthe publishedmeta-analysesinthe correctionsliteraturaend argue the importanceofspecificallymodellingheterogeneityand selectionbias. A sensitivityapproach is advocated, suggestingloweraverage effectsand substantially increasedmeasures {ofuncertaintyT}.he methodis testedon a medicalexamplewhereindependent confirmatiofnroma large controlledtrialis also available.},
	pages = {95--109},
	number = {1},
	journaltitle = {Journal of the Royal Statistical Society. Series A (Statistics in Society)},
	author = {Copas, John},
	date = {1999},
	langid = {english},
	file = {Copas - 1999 - What Works Selectivity Models and Meta-Analysis.pdf:/home/drosoneuro/Zotero/storage/6MBLMGCF/Copas - 1999 - What Works Selectivity Models and Meta-Analysis.pdf:application/pdf}
}

@article{cleary_application_1997,
	title = {An Application of Gibbs Sampling to Estimation in Meta-Analysis: Accounting for Publication Bias},
	volume = {22},
	pages = {141--154},
	number = {2},
	journaltitle = {Journal of Educational and Behavioral Statistics},
	author = {Cleary, Richard J and Casella, George},
	date = {1997},
	langid = {english},
	file = {Cleary and Casella - An Application of Gibbs Sampling to Estimation in .pdf:/home/drosoneuro/Zotero/storage/PNPCEVBY/Cleary and Casella - An Application of Gibbs Sampling to Estimation in .pdf:application/pdf}
}