\chapter{Conclusion}
\label{cha:conclusion_and_limitations}
\epigraph{\centering \textit{`In theory, practice is simple.'}}{--- Trygve M. H. Reenskaug}%\par Professor emeritus of informatics at the University of Oslo}
In this thesis, I presented an overview of a range of methods to construct robust and comparable measures for statistical evidence. In addition, I presented different approaches to detect and correct publication bias in effect size measures. Even though all of these methods and approaches are backed up by solid theoretical arguments, they often hinge on assumptions about the nature of scientific studies and the publication process. Assumptions which might be true but which might also be completely wrong.\par
For example, all tests introduced in this thesis as well as my adaptions thereof assume that individual studies are independent, that there is no between-study heterogeneity and that there is a fixed global effect. All these assumptions might be and are usually violated in real-life scientific practice.\par
Hence, it is important to keep in mind that none of the methods explained can be regarded as a `silver bullet' to fight publication bias---this is especially true, when one only relies on one of these methods alone. For example, it should have become clear that the reliance on the funnel plot as a diagnostic tool and the trim-and-fill approach as a corrective method is woefully inadequate to capture the wide range of forms in which publication bias can appear. Hence, it should become common practice to use more than one approach with different underlying assumptions to detect and correct publication bias.\par
In this spirit, I would like to pursue this topic with the following three goals in mind:
\begin{description}[leftmargin=!,labelwidth=\widthof{\bfseries $\boldsymbol{E_4}$}]
    \item [Systematic evaluation of performance] The scope of this did not allow me to systematically test the performance of all methods described. Hence, I would like to do this for a range of different scenarios of publication bias based on different assumptions and empirical observations. This is important since many of the published methods only perform well in very specific scenarios and completely fail if certain theoretical assumptions are not met. This also holds for my own methods presented in this thesis. It is therefore crucial to systematically asses the performance of all of these methods in order to construct reliable usage recommendations for statisticians and non-statisticians alike.\\
    \item[Extend assessment to additional methods] This thesis is not an exhaustive summary of methods pertaining to the detection and correction of publication bias. For example, I did not go into methods accounting for between-study heterogeneity and applying random effects models \citep{piao_copaslike_2018} or Bayesian approaches \citep{cleary_application_1997, andrews_identification_2017}, and hardly touched maximum likelihood-based approach such as the ones proposed by \citet{copas_what_1999}. Since these approaches often hinge on assumptions that differ from those underlying the methods described in this thesis, it is most likely advantageous to include them at a later step in order to increase the range of scenarios in which publication bias can be detected.\\
    \item [Creating ensemble models] My simulations have shown that many standard methods to detect and correct publication bias fail in certain situations and can even be further improved from a practical and sometimes even from a theoretical point of view. Given the fact that different methods hinge on different assumptions and thus show different performances depending on the concrete scenario, it should be clear that reliance on only one approach to detect or correct publication bias is prone to yield misleading results. It is therefore desirable to combine some of these methods into ensemble models for the detection and correction of publication bias. Since many of the methods described in this thesis are based on similar or related statistical measure, aggregating results across different methods should be possible.
\end{description}
However, I must point out the observation that the best statistical tools to correct biased estimates pale in comparison with non-statistical methods when it comes to efficacy to prevent bias in scientific results. For example, a strict distinction between exploratory and confirmatory studies---with mandatory pre-registration for the latter---could alleviate a lot of the problems outlined in this thesis. For a start, it would be possible to select only those studies that were intended to be confirmatory for meta-analysis and ignore the rest. In addition, it would be possible to calculate the publication probability of non-significant findings to correct for any kind of publication bias. And last but definitely not least it would help increasing the overall quality of scientific work if researchers are forced to think about study designs and statistical methods before the onset of the study---hopefully also by consulting a statistician before and not, as is often the case, after a study. Because, as \citet[p.~17]{fisher_presidential_1938} already remarked: `To consult a statistician after an experiment is finished is often merely to ask him to conduct a \textit{post mortem} examination. He can perhaps say what the experiment died of."
\vspace*{\fill}

%Assumptions that might and often are violated in real life:
%\begin{itemize}
    %\item Sample Variance is known
    %\item Sample is large enough for the central limit theorem to hold
    %\item Access to sample size, mean and sample standard deviation is given
%\end{itemize}

%Also, all my simulations of publication bias were based on hypothesis test using the variance stabilised transformation $V_n$ as described in Eq.~\ref{eq:Vn_stud}. However, in many cases researcher use the $z$-test instead or other inappropriate tests which increases the error rates
% .There also exist several techniques to model selection and correct for publication bias using weighted distribution theory (Hedges 1984; Iyengar and Greenhouse 1988; Hedges 1992; Dear and Begg 1992), Bayesian statistics (Cleary and Casella 1997), and maximum likelihood (Copas 1999). In addition to selection models, sensitivity analyses are often used to ascertain the severity of the bias (Rosenthal 1979; Gleser and Olkin 1996; Duval and Tweedie 2000) Stanley and Jarrell (1989) meta-regression analysis