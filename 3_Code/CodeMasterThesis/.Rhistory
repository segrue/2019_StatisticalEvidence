p1s
p1s <- seq(0.01, 0.99, by = 0.01)
start.time <- Sys.time()
# question: does transformation make sense in the theoretical case?
if (corr == T) {
# Anscombe estimator (includes continuity correction) p_anscombe <- (x+3/8)/(n+3/4)
p1_hats <- (sapply(p1s, function(x) rbinom(n_sim, n_study, x)) + 3 / 8) /
(n_study + 3 / 4)
p1s_th <- matrix(rep((p1s * n_study + 3 / 8) / (n_study + 3 / 4), n_sim),
n_sim, length(p1s), byrow = TRUE)
} else {
# MLE estimator without continuity correction p_hat <- x/n
p1_hats <- sapply(p1s, function(x) rbinom(n_sim, n_study, x)) / n_study
p1s_th <- matrix(rep(p1s, n_sim), n_sim, length(p1s), byrow = TRUE)
}
# calculate theoretical and empiral evidence based on z-statistic
# question: can I use empirical SE to calculate Zn_th > how is distribution of this statistic?
Zn_emp <- calc_evidence(mu0s = p0s, mu1s = p1_hats, n_study = n_study, func = z_stat_binom)
Zn_emp_avg <- avg_evidence(Zn_emp, p0s, p1s)
Zn_emp_sd <- sd_evidence(Zn_emp, p0s, p1s)
Zn_th_avg <- sapply(p1s, function(p1) z_stat_binom(p0s, p1, n_study))
Zn_th_sd <- matrix(1, nrow = dim(Zn_th_avg)[1], ncol = dim(Zn_th_avg)[2])
dim(p1_hats)
p1s <- 0.01
start.time <- Sys.time()
# question: does transformation make sense in the theoretical case?
if (corr == T) {
# Anscombe estimator (includes continuity correction) p_anscombe <- (x+3/8)/(n+3/4)
p1_hats <- (sapply(p1s, function(x) rbinom(n_sim, n_study, x)) + 3 / 8) /
(n_study + 3 / 4)
p1s_th <- matrix(rep((p1s * n_study + 3 / 8) / (n_study + 3 / 4), n_sim),
n_sim, length(p1s), byrow = TRUE)
} else {
# MLE estimator without continuity correction p_hat <- x/n
p1_hats <- sapply(p1s, function(x) rbinom(n_sim, n_study, x)) / n_study
p1s_th <- matrix(rep(p1s, n_sim), n_sim, length(p1s), byrow = TRUE)
}
# calculate theoretical and empiral evidence based on z-statistic
# question: can I use empirical SE to calculate Zn_th > how is distribution of this statistic?
Zn_emp <- calc_evidence(mu0s = p0s, mu1s = p1_hats, n_study = n_study, func = z_stat_binom)
Zn_emp_avg <- avg_evidence(Zn_emp, p0s, p1s)
Zn_emp_sd <- sd_evidence(Zn_emp, p0s, p1s)
Zn_th_avg <- sapply(p1s, function(p1) z_stat_binom(p0s, p1, n_study))
Zn_th_sd <- matrix(1, nrow = dim(Zn_th_avg)[1], ncol = dim(Zn_th_avg)[2])
p1_hats
p1s <- 0.2
start.time <- Sys.time()
# question: does transformation make sense in the theoretical case?
if (corr == T) {
# Anscombe estimator (includes continuity correction) p_anscombe <- (x+3/8)/(n+3/4)
p1_hats <- (sapply(p1s, function(x) rbinom(n_sim, n_study, x)) + 3 / 8) /
(n_study + 3 / 4)
p1s_th <- matrix(rep((p1s * n_study + 3 / 8) / (n_study + 3 / 4), n_sim),
n_sim, length(p1s), byrow = TRUE)
} else {
# MLE estimator without continuity correction p_hat <- x/n
p1_hats <- sapply(p1s, function(x) rbinom(n_sim, n_study, x)) / n_study
p1s_th <- matrix(rep(p1s, n_sim), n_sim, length(p1s), byrow = TRUE)
}
# calculate theoretical and empiral evidence based on z-statistic
# question: can I use empirical SE to calculate Zn_th > how is distribution of this statistic?
Zn_emp <- calc_evidence(mu0s = p0s, mu1s = p1_hats, n_study = n_study, func = z_stat_binom)
Zn_emp_avg <- avg_evidence(Zn_emp, p0s, p1s)
Zn_emp_sd <- sd_evidence(Zn_emp, p0s, p1s)
Zn_th_avg <- sapply(p1s, function(p1) z_stat_binom(p0s, p1, n_study))
Zn_th_sd <- matrix(1, nrow = dim(Zn_th_avg)[1], ncol = dim(Zn_th_avg)[2])
p1_hats
Zn_emp_avg
z_stat_binom(0,p1_hats,5)
z_stat_binom(0,p1_hats,5)[0:10]
p1_hats[0:10]
Zn
#MLE estimator without continuity correction p_hat <- x/n
p1_hats <- sapply(p1s, function(p1) rbinom(n_sim,n_study,p1))/n_study
p1_hats
Zn <- sapply(idx, function(j) z_stat_binom(p0,p1_hats[,j],n_study))
#calculate and save theoretical distributions Zn and Vn
# hypotheses:
# H0: p <= p0
# H1: p > p0
# Note: all calculations be
p1s <- c(0.1,0.5,0.9)
#MLE estimator without continuity correction p_hat <- x/n
p1_hats <- sapply(p1s, function(p1) rbinom(n_sim,n_study,p1))/n_study
Zn <- sapply(idx, function(j) z_stat_binom(p0,p1_hats[,j],n_study))
Zn
p1_hats[0:10]
Ã¼'
)
'
p0
p1_hats
p1_hats[1:10,]
Zn
Zn[1:10,]
Zn[1:4,]
p1_hats[1:4,]
1/(0*(1-0))
1/(1*(1-1))
sqrt((1*(1-1)))
0/sqrt((1*(1-1)))
1/sqrt((1*(1-1)))
0/sqrt((1*(1-1)))
Zn
Zn[1:4,]
p1_hats[1:4,]
fig3_plotter(evidence_binom[n_study==5,])
#calculate and save theoretical distributions Zn and Vn
# hypotheses:
# H0: p <= p0
# H1: p > p0
# Note: all calculations be
p1s <- c(0.1,0.5,0.9)
p0 <- 0
n_studies <- c(5,10,20,30)
probs <- seq(0.01,0.99,length.out=100)
n_sim <- 5e3
cols <- c("cdf","p1","quantile","id","n_study","p0")
seed <- 20190505
corr <- T
correction <- ifelse(corr,"Ans","MLE")
name <- paste0("binom_quantiles_",correction,"_",n_sim,"_",seed)
quantiles <- data.table(matrix(NA,nrow=0,ncol=6))
colnames(quantiles) <- cols
#need to adjust for the expectation of Vn!! E(Tn_vst) = sqrt(n)*K(theta)
set.seed(seed)
for (i in 1:length(n_studies)){
idx <- 1:length(p1s)
n_study <- n_studies[i]
ses <- sqrt(p1s*(1-p1s)/n_study)
mus_Zn <- (p1s-p0)/ses # expected value of Zn
mus_Vn <- vst_binom(p0,p1s,n_study) #exptected value of Vn
diffs <- mus_Zn-mus_Vn
#Calculate theoretical quantiles of normal distribution with mean mu_Zn and Var = 1
Qnorms_th <- dat_transform_quantiles((sapply(mus_Zn,
function(mu) qnorm(probs,mu,1))),"th",n_study)
# calculate estimator for p1
if (corr==T){
#Anscombe estimator (includes continuity correction) p_anscombe <- (x+3/8)/(n+3/4)
p1_hats <- (sapply(p1s,function(x) rbinom(n_sim,n_study,x))+3/8)/(n_study+3/4)
} else {
#MLE estimator without continuity correction p_hat <- x/n
p1_hats <- sapply(p1s, function(p1) rbinom(n_sim,n_study,p1))/n_study
p1_hats[p1_hats==0] <- 1e-3 #add small value to prevent infinity values
}
# calculate quantiles of Vn and Zn
# difference (diffs <- mus_Zn-mus_Vn) needs to be added so that Vn and Zn
# are centered around the same mean to facilitate comparison of normalty
Vn <- sapply(idx, function(j) vst_binom(p0,p1_hats[,j],n_study)+diffs[j])
Zn <- sapply(idx, function(j) z_stat_binom(p0,p1_hats[,j],n_study))
#replace infinity values by NA
Zn[is.infinite(Zn)] <- NA
#replace infinity values with 999 or -999 to enable calculation
Zn[is.infinite(Zn)] <- 999*sign(Zn[is.infinite(Zn)])
Zn_q <- dat_transform_quantiles((sapply(idx, function(j) quantile(Zn[,j],probs,na.rm=T))),"Zn",n_study)
Vn_q <- dat_transform_quantiles((sapply(idx, function(j) quantile(Vn[,j],probs,na.rm=T))),"Vn",n_study)
quantiles <- rbind(quantiles,Qnorms_th,Zn_q,Vn_q)
}
save(quantiles,file=paste0("data/",name,".RData"))
#calculate and save theoretical distributions Zn and Vn
# hypotheses:
# H0: p <= p0
# H1: p > p0
# Note: all calculations be
p1s <- c(0.1,0.5,0.9)
p0 <- 0
n_studies <- c(5,10,20,30)
probs <- seq(0.01,0.99,length.out=100)
n_sim <- 5e3
cols <- c("cdf","p1","quantile","id","n_study","p0")
seed <- 20190505
corr <- F
correction <- ifelse(corr,"Ans","MLE")
name <- paste0("binom_quantiles_",correction,"_",n_sim,"_",seed)
quantiles <- data.table(matrix(NA,nrow=0,ncol=6))
colnames(quantiles) <- cols
#need to adjust for the expectation of Vn!! E(Tn_vst) = sqrt(n)*K(theta)
set.seed(seed)
for (i in 1:length(n_studies)){
idx <- 1:length(p1s)
n_study <- n_studies[i]
ses <- sqrt(p1s*(1-p1s)/n_study)
mus_Zn <- (p1s-p0)/ses # expected value of Zn
mus_Vn <- vst_binom(p0,p1s,n_study) #exptected value of Vn
diffs <- mus_Zn-mus_Vn
#Calculate theoretical quantiles of normal distribution with mean mu_Zn and Var = 1
Qnorms_th <- dat_transform_quantiles((sapply(mus_Zn,
function(mu) qnorm(probs,mu,1))),"th",n_study)
# calculate estimator for p1
if (corr==T){
#Anscombe estimator (includes continuity correction) p_anscombe <- (x+3/8)/(n+3/4)
p1_hats <- (sapply(p1s,function(x) rbinom(n_sim,n_study,x))+3/8)/(n_study+3/4)
} else {
#MLE estimator without continuity correction p_hat <- x/n
p1_hats <- sapply(p1s, function(p1) rbinom(n_sim,n_study,p1))/n_study
p1_hats[p1_hats==0] <- 1e-3 #add small value to prevent infinity values
}
# calculate quantiles of Vn and Zn
# difference (diffs <- mus_Zn-mus_Vn) needs to be added so that Vn and Zn
# are centered around the same mean to facilitate comparison of normalty
Vn <- sapply(idx, function(j) vst_binom(p0,p1_hats[,j],n_study)+diffs[j])
Zn <- sapply(idx, function(j) z_stat_binom(p0,p1_hats[,j],n_study))
#replace infinity values by NA
Zn[is.infinite(Zn)] <- NA
#replace infinity values with 999 or -999 to enable calculation
Zn[is.infinite(Zn)] <- 999*sign(Zn[is.infinite(Zn)])
Zn_q <- dat_transform_quantiles((sapply(idx, function(j) quantile(Zn[,j],probs,na.rm=T))),"Zn",n_study)
Vn_q <- dat_transform_quantiles((sapply(idx, function(j) quantile(Vn[,j],probs,na.rm=T))),"Vn",n_study)
quantiles <- rbind(quantiles,Qnorms_th,Zn_q,Vn_q)
}
save(quantiles,file=paste0("data/",name,".RData"))
### Figure 2.2: Normal fit of Zn & Vn based on binomial ------------------------
### (fig:normal_fit_binomial_[Corr])
corr <- "Ans" # either "MLE" for no correction or "Ans" vor Anscombe correction
# define figure name
figname <- paste0("ch2_fig2_normal_fit_binomial_",corr)
#load data
load(paste0(in_path,"binom_quantiles_",corr,"_5000_20190505.RData"),
verbose=TRUE)
# variables are
# cdf: cumulative probability
# p0: p under H0
# p1: p under H1
# quantile: x-value corresponding to cumulative probability prob
# id: th = cdf of the normal distribution with mu = ; Zn = cdf of z-score based on binomial
#     Vn = cdf of variance stabilised evidence measure Vn
# n_study: number of samples per simulated study
#plotting multiple plots in one window: https://cran.r-project.org/web/packages/egg/vignettes/Ecosystem.html
#n=5
cdf_plotter <- function(dat) {
q_min <- min(dat[id=="th",]$quantile)
q_max <- max(dat[id=="th",]$quantile)
cdf_plot <- ggplot(data=dat,aes(x=quantile,y=cdf, col=id,
group=interaction(p1,id),linetype=factor(p1))) +
geom_line() + scale_color_manual(name = "F(x):",
labels = c(bquote(Phi(x-E*"["*Z[n]*"]")),bquote(Z[n]),
bquote(V[n]-E*"["*V[n]*"]"+E*"["*Z[n]*"]")),
values=c("th"=2,"Zn"=3,"Vn"=4))+
scale_linetype_manual(name = bquote(H[1]~":"),
labels = c(bquote(p==0.1), bquote(p==0.5),
bquote(p==0.9)),
values = c("solid","dashed","dotdash")) +
ylim(0,1) + ggtitle(bquote("x"==.(dat$n_study[1]))) +
labs(x= "x", y = "F(x)") + coord_cartesian(xlim=c(q_min,q_max)) +
theme(text=element_text(size=font_size, family=font_family),
plot.title = element_text(size=font_size))
}
qq_plotter <- function(dat){
q_min <- min(dat[id=="th",]$quantile)
q_max <- max(dat[id=="th",]$quantile)
th_quantiles <- qnorm(seq(0.01,0.99,
along.with=dat[id=="th" & p1==0.1,quantile]),0,1)
dat$q_theoretical <- rep(th_quantiles,dim(unique(dat[,.(id,p1)]))[1])
q_plot <- ggplot(data=dat,aes(x = q_theoretical, y= quantile, col=id,
group=interaction(p1,id),linetype=factor(p1))) +
geom_line() + scale_color_manual(values=c("th"=2,"Zn"=3,"Vn"=4)) +
scale_linetype_manual(values = c("solid","dashed","dotdash")) +
labs(x= bquote(Phi^{-1}*(x)), y = bquote(F^{-1}*(x))) +
coord_cartesian(ylim=c(q_min,q_max)) +
theme(text=element_text(size=font_size, family=font_family),
legend.position="none")
}
# plot all figures (two for each n_study) on the same page
p1 <- cdf_plotter(quantiles[quantiles$n_study==5,])
p2 <- qq_plotter(quantiles[quantiles$n_study==5,])
p3 <- cdf_plotter(quantiles[quantiles$n_study==10,])
p4 <- qq_plotter(quantiles[quantiles$n_study==10,])
p5 <- cdf_plotter(quantiles[quantiles$n_study==20,])
p6 <- qq_plotter(quantiles[quantiles$n_study==20,])
#p7 <- cdf_plotter(quantiles[quantiles$n_study==30,])
#p8 <- qq_plotter(quantiles[quantiles$n_study==30,])
fig2 <- ggarrange(p1,p2,p3,p4,p5,p6,ncol=2,nrow=3,
align="hv",legend="top",common.legend = T,
labels = c("A", "","B","","C",""))
ggsave(filename= paste0(out_path,figname,".pdf"),plot=fig2,
width=A4[1], height=0.9*A4[2], device = cairo_pdf)
### Figure 2.2: Normal fit of Zn & Vn based on binomial ------------------------
### (fig:normal_fit_binomial_[Corr])
corr <- "MLE" # either "MLE" for no correction or "Ans" vor Anscombe correction
# define figure name
figname <- paste0("ch2_fig2_normal_fit_binomial_",corr)
#load data
load(paste0(in_path,"binom_quantiles_",corr,"_5000_20190505.RData"),
verbose=TRUE)
# variables are
# cdf: cumulative probability
# p0: p under H0
# p1: p under H1
# quantile: x-value corresponding to cumulative probability prob
# id: th = cdf of the normal distribution with mu = ; Zn = cdf of z-score based on binomial
#     Vn = cdf of variance stabilised evidence measure Vn
# n_study: number of samples per simulated study
#plotting multiple plots in one window: https://cran.r-project.org/web/packages/egg/vignettes/Ecosystem.html
#n=5
cdf_plotter <- function(dat) {
q_min <- min(dat[id=="th",]$quantile)
q_max <- max(dat[id=="th",]$quantile)
cdf_plot <- ggplot(data=dat,aes(x=quantile,y=cdf, col=id,
group=interaction(p1,id),linetype=factor(p1))) +
geom_line() + scale_color_manual(name = "F(x):",
labels = c(bquote(Phi(x-E*"["*Z[n]*"]")),bquote(Z[n]),
bquote(V[n]-E*"["*V[n]*"]"+E*"["*Z[n]*"]")),
values=c("th"=2,"Zn"=3,"Vn"=4))+
scale_linetype_manual(name = bquote(H[1]~":"),
labels = c(bquote(p==0.1), bquote(p==0.5),
bquote(p==0.9)),
values = c("solid","dashed","dotdash")) +
ylim(0,1) + ggtitle(bquote("x"==.(dat$n_study[1]))) +
labs(x= "x", y = "F(x)") + coord_cartesian(xlim=c(q_min,q_max)) +
theme(text=element_text(size=font_size, family=font_family),
plot.title = element_text(size=font_size))
}
qq_plotter <- function(dat){
q_min <- min(dat[id=="th",]$quantile)
q_max <- max(dat[id=="th",]$quantile)
th_quantiles <- qnorm(seq(0.01,0.99,
along.with=dat[id=="th" & p1==0.1,quantile]),0,1)
dat$q_theoretical <- rep(th_quantiles,dim(unique(dat[,.(id,p1)]))[1])
q_plot <- ggplot(data=dat,aes(x = q_theoretical, y= quantile, col=id,
group=interaction(p1,id),linetype=factor(p1))) +
geom_line() + scale_color_manual(values=c("th"=2,"Zn"=3,"Vn"=4)) +
scale_linetype_manual(values = c("solid","dashed","dotdash")) +
labs(x= bquote(Phi^{-1}*(x)), y = bquote(F^{-1}*(x))) +
coord_cartesian(ylim=c(q_min,q_max)) +
theme(text=element_text(size=font_size, family=font_family),
legend.position="none")
}
# plot all figures (two for each n_study) on the same page
p1 <- cdf_plotter(quantiles[quantiles$n_study==5,])
p2 <- qq_plotter(quantiles[quantiles$n_study==5,])
p3 <- cdf_plotter(quantiles[quantiles$n_study==10,])
p4 <- qq_plotter(quantiles[quantiles$n_study==10,])
p5 <- cdf_plotter(quantiles[quantiles$n_study==20,])
p6 <- qq_plotter(quantiles[quantiles$n_study==20,])
#p7 <- cdf_plotter(quantiles[quantiles$n_study==30,])
#p8 <- qq_plotter(quantiles[quantiles$n_study==30,])
fig2 <- ggarrange(p1,p2,p3,p4,p5,p6,ncol=2,nrow=3,
align="hv",legend="top",common.legend = T,
labels = c("A", "","B","","C",""))
ggsave(filename= paste0(out_path,figname,".pdf"),plot=fig2,
width=A4[1], height=0.9*A4[2], device = cairo_pdf)
Zn
p1_hats
Zn
p1_hats[0:10,]
i <- 1
idx <- 1:length(p1s)
n_study <- n_studies[i]
ses <- sqrt(p1s*(1-p1s)/n_study)
mus_Zn <- (p1s-p0)/ses # expected value of Zn
mus_Vn <- vst_binom(p0,p1s,n_study) #exptected value of Vn
diffs <- mus_Zn-mus_Vn
#Calculate theoretical quantiles of normal distribution with mean mu_Zn and Var = 1
Qnorms_th <- dat_transform_quantiles((sapply(mus_Zn,
function(mu) qnorm(probs,mu,1))),"th",n_study)
# calculate estimator for p1
if (corr==T){
#Anscombe estimator (includes continuity correction) p_anscombe <- (x+3/8)/(n+3/4)
p1_hats <- (sapply(p1s,function(x) rbinom(n_sim,n_study,x))+3/8)/(n_study+3/4)
} else {
#MLE estimator without continuity correction p_hat <- x/n
p1_hats <- sapply(p1s, function(p1) rbinom(n_sim,n_study,p1))/n_study
p1_hats[p1_hats==0] <- 1e-3 #add small value to prevent infinity values
}
# calculate quantiles of Vn and Zn
# difference (diffs <- mus_Zn-mus_Vn) needs to be added so that Vn and Zn
# are centered around the same mean to facilitate comparison of normalty
Vn <- sapply(idx, function(j) vst_binom(p0,p1_hats[,j],n_study)+diffs[j])
Zn <- sapply(idx, function(j) z_stat_binom(p0,p1_hats[,j],n_study))
Zn[0:10,]
p1_hats
p1_hats[0:4,]
Zn[0:4,]
p1s <- c(0.1,0.5,0.9)
p0 <- 0
n_studies <- c(5,10,20,30)
probs <- seq(0.01,0.99,length.out=100)
n_sim <- 5e3
cols <- c("cdf","p1","quantile","id","n_study","p0")
seed <- 20190505
corr <- F
correction <- ifelse(corr,"Ans","MLE")
name <- paste0("binom_quantiles_",correction,"_",n_sim,"_",seed)
quantiles <- data.table(matrix(NA,nrow=0,ncol=6))
colnames(quantiles) <- cols
#need to adjust for the expectation of Vn!! E(Tn_vst) = sqrt(n)*K(theta)
set.seed(seed)
for (i in 1:length(n_studies)){
idx <- 1:length(p1s)
n_study <- n_studies[i]
ses <- sqrt(p1s*(1-p1s)/n_study)
mus_Zn <- (p1s-p0)/ses # expected value of Zn
mus_Vn <- vst_binom(p0,p1s,n_study) #exptected value of Vn
diffs <- mus_Zn-mus_Vn
#Calculate theoretical quantiles of normal distribution with mean mu_Zn and Var = 1
Qnorms_th <- dat_transform_quantiles((sapply(mus_Zn,
function(mu) qnorm(probs,mu,1))),"th",n_study)
# calculate estimator for p1
if (corr==T){
#Anscombe estimator (includes continuity correction) p_anscombe <- (x+3/8)/(n+3/4)
p1_hats <- (sapply(p1s,function(x) rbinom(n_sim,n_study,x))+3/8)/(n_study+3/4)
} else {
#MLE estimator without continuity correction p_hat <- x/n
p1_hats <- sapply(p1s, function(p1) rbinom(n_sim,n_study,p1))/n_study
p1_hats[p1_hats==0] <- 1e-6 #add small value to prevent infinity values
p1_hats[p1_hats==1] <- 1-1e-6 #remove small value to prevent infinity values
}
# calculate quantiles of Vn and Zn
# difference (diffs <- mus_Zn-mus_Vn) needs to be added so that Vn and Zn
# are centered around the same mean to facilitate comparison of normalty
Vn <- sapply(idx, function(j) vst_binom(p0,p1_hats[,j],n_study)+diffs[j])
Zn <- sapply(idx, function(j) z_stat_binom(p0,p1_hats[,j],n_study))
#replace infinity values by NA
Zn[is.infinite(Zn)] <- NA
#replace infinity values with 999 or -999 to enable calculation
#Zn[is.infinite(Zn)] <- 999*sign(Zn[is.infinite(Zn)])
Zn_q <- dat_transform_quantiles((sapply(idx, function(j) quantile(Zn[,j],probs,na.rm=T))),"Zn",n_study)
Vn_q <- dat_transform_quantiles((sapply(idx, function(j) quantile(Vn[,j],probs,na.rm=T))),"Vn",n_study)
quantiles <- rbind(quantiles,Qnorms_th,Zn_q,Vn_q)
}
save(quantiles,file=paste0("data/",name,".RData"))
### Figure 2.2: Normal fit of Zn & Vn based on binomial ------------------------
### (fig:normal_fit_binomial_[Corr])
corr <- "MLE" # either "MLE" for no correction or "Ans" vor Anscombe correction
# define figure name
figname <- paste0("ch2_fig2_normal_fit_binomial_",corr)
#load data
load(paste0(in_path,"binom_quantiles_",corr,"_5000_20190505.RData"),
verbose=TRUE)
# variables are
# cdf: cumulative probability
# p0: p under H0
# p1: p under H1
# quantile: x-value corresponding to cumulative probability prob
# id: th = cdf of the normal distribution with mu = ; Zn = cdf of z-score based on binomial
#     Vn = cdf of variance stabilised evidence measure Vn
# n_study: number of samples per simulated study
#plotting multiple plots in one window: https://cran.r-project.org/web/packages/egg/vignettes/Ecosystem.html
#n=5
cdf_plotter <- function(dat) {
q_min <- min(dat[id=="th",]$quantile)
q_max <- max(dat[id=="th",]$quantile)
cdf_plot <- ggplot(data=dat,aes(x=quantile,y=cdf, col=id,
group=interaction(p1,id),linetype=factor(p1))) +
geom_line() + scale_color_manual(name = "F(x):",
labels = c(bquote(Phi(x-E*"["*Z[n]*"]")),bquote(Z[n]),
bquote(V[n]-E*"["*V[n]*"]"+E*"["*Z[n]*"]")),
values=c("th"=2,"Zn"=3,"Vn"=4))+
scale_linetype_manual(name = bquote(H[1]~":"),
labels = c(bquote(p==0.1), bquote(p==0.5),
bquote(p==0.9)),
values = c("solid","dashed","dotdash")) +
ylim(0,1) + ggtitle(bquote("x"==.(dat$n_study[1]))) +
labs(x= "x", y = "F(x)") + coord_cartesian(xlim=c(q_min,q_max)) +
theme(text=element_text(size=font_size, family=font_family),
plot.title = element_text(size=font_size))
}
qq_plotter <- function(dat){
q_min <- min(dat[id=="th",]$quantile)
q_max <- max(dat[id=="th",]$quantile)
th_quantiles <- qnorm(seq(0.01,0.99,
along.with=dat[id=="th" & p1==0.1,quantile]),0,1)
dat$q_theoretical <- rep(th_quantiles,dim(unique(dat[,.(id,p1)]))[1])
q_plot <- ggplot(data=dat,aes(x = q_theoretical, y= quantile, col=id,
group=interaction(p1,id),linetype=factor(p1))) +
geom_line() + scale_color_manual(values=c("th"=2,"Zn"=3,"Vn"=4)) +
scale_linetype_manual(values = c("solid","dashed","dotdash")) +
labs(x= bquote(Phi^{-1}*(x)), y = bquote(F^{-1}*(x))) +
coord_cartesian(ylim=c(q_min,q_max)) +
theme(text=element_text(size=font_size, family=font_family),
legend.position="none")
}
# plot all figures (two for each n_study) on the same page
p1 <- cdf_plotter(quantiles[quantiles$n_study==5,])
p2 <- qq_plotter(quantiles[quantiles$n_study==5,])
p3 <- cdf_plotter(quantiles[quantiles$n_study==10,])
p4 <- qq_plotter(quantiles[quantiles$n_study==10,])
p5 <- cdf_plotter(quantiles[quantiles$n_study==20,])
p6 <- qq_plotter(quantiles[quantiles$n_study==20,])
#p7 <- cdf_plotter(quantiles[quantiles$n_study==30,])
#p8 <- qq_plotter(quantiles[quantiles$n_study==30,])
fig2 <- ggarrange(p1,p2,p3,p4,p5,p6,ncol=2,nrow=3,
align="hv",legend="top",common.legend = T,
labels = c("A", "","B","","C",""))
ggsave(filename= paste0(out_path,figname,".pdf"),plot=fig2,
width=A4[1], height=0.9*A4[2], device = cairo_pdf)
i <- 1
idx <- 1:length(p1s)
n_study <- n_studies[i]
ses <- sqrt(p1s*(1-p1s)/n_study)
mus_Zn <- (p1s-p0)/ses # expected value of Zn
mus_Vn <- vst_binom(p0,p1s,n_study) #exptected value of Vn
diffs <- mus_Zn-mus_Vn
#Calculate theoretical quantiles of normal distribution with mean mu_Zn and Var = 1
Qnorms_th <- dat_transform_quantiles((sapply(mus_Zn,
function(mu) qnorm(probs,mu,1))),"th",n_study)
# calculate estimator for p1
if (corr==T){
#Anscombe estimator (includes continuity correction) p_anscombe <- (x+3/8)/(n+3/4)
p1_hats <- (sapply(p1s,function(x) rbinom(n_sim,n_study,x))+3/8)/(n_study+3/4)
} else {
#MLE estimator without continuity correction p_hat <- x/n
p1_hats <- sapply(p1s, function(p1) rbinom(n_sim,n_study,p1))/n_study
p1_hats[p1_hats==0] <- 1e-6 #add small value to prevent infinity values
p1_hats[p1_hats==1] <- 1-1e-6 #remove small value to prevent infinity values
}
# calculate quantiles of Vn and Zn
# difference (diffs <- mus_Zn-mus_Vn) needs to be added so that Vn and Zn
# are centered around the same mean to facilitate comparison of normalty
Vn <- sapply(idx, function(j) vst_binom(p0,p1_hats[,j],n_study)+diffs[j])
Zn <- sapply(idx, function(j) z_stat_binom(p0,p1_hats[,j],n_study))
Zn[1:10,]
