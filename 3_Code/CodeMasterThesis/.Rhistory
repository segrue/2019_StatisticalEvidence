t_obs=abs(t_obs) #Take absolute value of t-value (p-curve assumes same sign and/or sign does not matter)
p_obs=2*(1-pt(t_obs,df=df_obs)) #Compute p-values of each t in t_obs so as to keep only p<.05 results
t.sig=subset(t_obs,p_obs<.05) #Significant t-values
df.sig=subset(df_obs,p_obs<.05) #d.f. associated with significant t.values
ncp_est=sqrt((df.sig+2)/4)*d_est #Compute noncentrality parameter for that sample size and candidate effect size
tc=qt(.975,df.sig) #Compute critical t-value to get p=.05
power_est=1-pt(tc,df.sig,ncp_est) #Compute power for obtaining that t-value or bigger, given the noncentrality parameter
p_larger=pt(t.sig,df=df.sig,ncp=ncp_est) #Probability of obtaining a t-value bigger than the one that is observed (this is a vector)
ppr=(p_larger-(1-power_est))/power_est #Conditional probability of larger t-value given that it is p<.05, pp-values
KSD=ks.test(ppr,punif)$statistic #Kolmogorov Smirnov test on that vector against the theoretical U[0,1] distribution
return(KSD) }
plotloss=function(t_obs,df_obs,dmin,dmax) { loss.all=c() #Vector where results of fit for each candidate effect size are stored
di=c() #Vector where the respective effect sizes are stored
for (i in 0:((dmax-dmin)*100)) { #Do a loop considering every effect size between dmin and dmax in steps of .01
d=dmin+i/100 #What effect size are we considering?
di=c(di,d) #Add it to the vector of effect sizes
options(warn=-1) #turn off warning because R often generates warnings when using noncentral pt() and
qt() that are inconsequential (they involve lack of precision at a degree where precision lacks practical relevance)
loss.all=c(loss.all,loss(df_obs=df_obs,t_obs=t_obs,d_est=d)) #add loss for that effect size to the vector with all losses
options(warn=0) #turn warnings back on
}
imin=match(min(loss.all),loss.all) #Find the attempted effect size that leads to smallest los overall
dstart=dmin+imin/100 #Counting from dmin, what effect size is that?
dhat=optimize(loss,c(dstart-.1,
dstart+.1), df_obs=df_obs,t_obs=t_obs) #Now optimize in the neighborhood of that effect size
#PLOT RESULTS
plot(di,loss.all,xlab=”Effect size\nCohen-d”, ylab=”Loss (D stat in KS test)”,ylim=c(0,1), main=”How well does each effect size fit?
(lower is better)”)
points(dhat$minimum,dhat$objective,pch=19,col=”red”,cex=2) #Put a red dot in the estimated effect size
#Add a label
text(dhat$minimum,dhat$objective-.08,paste0(“p-curve’s estimate of effect size:\nd=”,round(dhat$minimum,3)),col=”red”)
return(dhat$minimum)
}
#Find the best fitting effect size (this also generates a diagnostic plot)
plotloss=function(t_obs,df_obs,dmin,dmax) { loss.all=c() #Vector where results of fit for each candidate effect size are stored
di=c() #Vector where the respective effect sizes are stored
for (i in 0:((dmax-dmin)*100)) { #Do a loop considering every effect size between dmin and dmax in steps of .01
d=dmin+i/100 #What effect size are we considering?
di=c(di,d) #Add it to the vector of effect sizes
options(warn=-1) #turn off warning because R often generates warnings when using noncentral pt() and
qt() that are inconsequential (they involve lack of precision at a degree where precision lacks practical relevance)
loss.all=c(loss.all,loss(df_obs=df_obs,t_obs=t_obs,d_est=d)) #add loss for that effect size to the vector with all losses
options(warn=0) #turn warnings back on
}
imin=match(min(loss.all),loss.all) #Find the attempted effect size that leads to smallest los overall
dstart=dmin+imin/100 #Counting from dmin, what effect size is that?
dhat=optimize(loss,c(dstart-.1,
dstart+.1), df_obs=df_obs,t_obs=t_obs) #Now optimize in the neighborhood of that effect size
#PLOT RESULTS
plot(di,loss.all,xlab=”Effect size\nCohen-d”, ylab=”Loss (D stat in KS test)”,ylim=c(0,1), main=”How well does each effect size fit?
(lower is better)”)
points(dhat$minimum,dhat$objective,pch=19,col=”red”,cex=2) #Put a red dot in the estimated effect size
#Add a label
#text(dhat$minimum,dhat$objective-.08,paste0(“p-curve’s estimate of effect size:\nd=”,round(dhat$minimum,3)),col=”red”)
return(dhat$minimum)
}
#Find the best fitting effect size (this also generates a diagnostic plot)
plotloss=function(t_obs,df_obs,dmin,dmax) { loss.all=c() #Vector where results of fit for each candidate effect size are stored
di=c() #Vector where the respective effect sizes are stored
for (i in 0:((dmax-dmin)*100)) { #Do a loop considering every effect size between dmin and dmax in steps of .01
d=dmin+i/100 #What effect size are we considering?
di=c(di,d) #Add it to the vector of effect sizes
options(warn=-1) #turn off warning because R often generates warnings when using noncentral pt() and
qt() that are inconsequential (they involve lack of precision at a degree where precision lacks practical relevance)
loss.all=c(loss.all,loss(df_obs=df_obs,t_obs=t_obs,d_est=d)) #add loss for that effect size to the vector with all losses
options(warn=0) #turn warnings back on
}
imin=match(min(loss.all),loss.all) #Find the attempted effect size that leads to smallest los overall
dstart=dmin+imin/100 #Counting from dmin, what effect size is that?
dhat=optimize(loss,c(dstart-.1,
dstart+.1), df_obs=df_obs,t_obs=t_obs) #Now optimize in the neighborhood of that effect size
#PLOT RESULTS
plot(di,loss.all,xlab=”Effect size\nCohen-d”, ylab=”Loss (D stat in KS test)”,ylim=c(0,1), main=”How well does each effect size fit?
(lower is better)”)
points(dhat$minimum,dhat$objective,pch=19,col=”red”,cex=2) #Put a red dot in the estimated effect size
#Add a label
#text(dhat$minimum,dhat$objective-.08,paste0(“p-curve’s estimate of effect size:\nd=”,round(dhat$minimum,3)),col=”red”)
return(dhat$minimum)
}
#Example
t_obs= c(1.7, 2.8, -3.1, 2.4) # include one p > .05 and one negative t-value to highlight how we treat those
df_obs=c(44, 75, 125, 200)
plotloss(t_obs=t_obs,df_obs=df_obs,dmin=-1,dmax=1)
#Find the best fitting effect size (this also generates a diagnostic plot)
plotloss=function(t_obs,df_obs,dmin,dmax) {
loss.all=c() #Vector where results of fit for each candidate effect size are stored
di=c() #Vector where the respective effect sizes are stored
for (i in 0:((dmax-dmin)*100)) { #Do a loop considering every effect size between dmin and dmax in steps of .01
d=dmin+i/100 #What effect size are we considering?
di=c(di,d) #Add it to the vector of effect sizes
options(warn=-1) #turn off warning because R often generates warnings when using noncentral pt() and
qt() that are inconsequential (they involve lack of precision at a degree where precision lacks practical relevance)
loss.all=c(loss.all,loss(df_obs=df_obs,t_obs=t_obs,d_est=d)) #add loss for that effect size to the vector with all losses
options(warn=0) #turn warnings back on
}
imin=match(min(loss.all),loss.all) #Find the attempted effect size that leads to smallest los overall
dstart=dmin+imin/100 #Counting from dmin, what effect size is that?
dhat=optimize(loss,c(dstart-.1,
dstart+.1), df_obs=df_obs,t_obs=t_obs) #Now optimize in the neighborhood of that effect size
#PLOT RESULTS
plot(di,loss.all,xlab=”Effect size\nCohen-d”, ylab=”Loss (D stat in KS test)”,ylim=c(0,1), main=”How well does each effect size fit?
(lower is better)”)
points(dhat$minimum,dhat$objective,pch=19,col=”red”,cex=2) #Put a red dot in the estimated effect size
#Add a label
#text(dhat$minimum,dhat$objective-.08,paste0(“p-curve’s estimate of effect size:\nd=”,round(dhat$minimum,3)),col=”red”)
return(dhat$minimum)
}
#Find the best fitting effect size (this also generates a diagnostic plot)
plotloss=function(t_obs,df_obs,dmin,dmax) {
loss.all=c() #Vector where results of fit for each candidate effect size are stored
di=c() #Vector where the respective effect sizes are stored
for (i in 0:((dmax-dmin)*100)) { #Do a loop considering every effect size between dmin and dmax in steps of .01
d=dmin+i/100 #What effect size are we considering?
di=c(di,d) #Add it to the vector of effect sizes
options(warn=-1) #turn off warning because R often generates warnings when using noncentral pt() and
qt() that are inconsequential (they involve lack of precision at a degree where precision lacks practical relevance)
loss.all=c(loss.all,loss(df_obs=df_obs,t_obs=t_obs,d_est=d)) #add loss for that effect size to the vector with all losses
options(warn=0) #turn warnings back on
}
imin=match(min(loss.all),loss.all) #Find the attempted effect size that leads to smallest los overall
dstart=dmin+imin/100 #Counting from dmin, what effect size is that?
dhat=optimize(loss,c(dstart-.1,
dstart+.1), df_obs=df_obs,t_obs=t_obs) #Now optimize in the neighborhood of that effect size
#PLOT RESULTS
plot(di,loss.all,xlab=”Effect size\nCohen-d”, ylab=”Loss (D stat in KS test)”,ylim=c(0,1), main=”How well does each effect size fit?
(lower is better)”)
points(dhat$minimum,dhat$objective,pch=19,col=”red”,cex=2) #Put a red dot in the estimated effect size
#Add a label
text(dhat$minimum,dhat$objective-.08,paste0(“p-curve’s estimate of effect size:\nd=”,round(dhat$minimum,3)),col=”red”)
return(dhat$minimum)
}
#Find the best fitting effect size (this also generates a diagnostic plot)
plotloss=function(t_obs,df_obs,dmin,dmax) {
loss.all=c() #Vector where results of fit for each candidate effect size are stored
di=c() #Vector where the respective effect sizes are stored
for (i in 0:((dmax-dmin)*100)) { #Do a loop considering every effect size between dmin and dmax in steps of .01
d=dmin+i/100 #What effect size are we considering?
di=c(di,d) #Add it to the vector of effect sizes
#options(warn=-1) #turn off warning because R often generates warnings when using noncentral pt() and
qt() that are inconsequential (they involve lack of precision at a degree where precision lacks practical relevance)
loss.all=c(loss.all,loss(df_obs=df_obs,t_obs=t_obs,d_est=d)) #add loss for that effect size to the vector with all losses
options(warn=0) #turn warnings back on
}
imin=match(min(loss.all),loss.all) #Find the attempted effect size that leads to smallest los overall
dstart=dmin+imin/100 #Counting from dmin, what effect size is that?
dhat=optimize(loss,c(dstart-.1,
dstart+.1), df_obs=df_obs,t_obs=t_obs) #Now optimize in the neighborhood of that effect size
#PLOT RESULTS
plot(di,loss.all,xlab=”Effect size\nCohen-d”, ylab=”Loss (D stat in KS test)”,ylim=c(0,1), main=”How well does each effect size fit?
(lower is better)”)
points(dhat$minimum,dhat$objective,pch=19,col=”red”,cex=2) #Put a red dot in the estimated effect size
#Add a label
text(dhat$minimum,dhat$objective-.08,paste0(“p-curve’s estimate of effect size:\nd=”,round(dhat$minimum,3)),col=”red”)
return(dhat$minimum)
}
loss.all=c() #Vector where results of fit for each candidate effect size are stored
#Find the best fitting effect size (this also generates a diagnostic plot)
plotloss=function(t_obs,df_obs,dmin,dmax) {
loss.all=c() #Vector where results of fit for each candidate effect size are stored
di=c() #Vector where the respective effect sizes are stored
for (i in 0:((dmax-dmin)*100)) { #Do a loop considering every effect size between dmin and dmax in steps of .01
d=dmin+i/100 #What effect size are we considering?
di=c(di,d) #Add it to the vector of effect sizes
#options(warn=-1) #turn off warning because R often generates warnings when using noncentral pt() and
qt() that are inconsequential (they involve lack of precision at a degree where precision lacks practical relevance)
loss.all=c(loss.all,loss(df_obs=df_obs,t_obs=t_obs,d_est=d)) #add loss for that effect size to the vector with all losses
options(warn=0) #turn warnings back on
}
imin=match(min(loss.all),loss.all) #Find the attempted effect size that leads to smallest los overall
dstart=dmin+imin/100 #Counting from dmin, what effect size is that?
dhat=optimize(loss,c(dstart-.1,
dstart+.1), df_obs=df_obs,t_obs=t_obs) #Now optimize in the neighborhood of that effect size
#PLOT RESULTS
plot(di,loss.all,xlab=”Effect size\nCohen-d”, ylab=”Loss (D stat in KS test)”,ylim=c(0,1), main=”How well does each effect size fit?
(lower is better)”)
points(dhat$minimum,dhat$objective,pch=19,col=”red”,cex=2) #Put a red dot in the estimated effect size
#Add a label
text(dhat$minimum,dhat$objective-.08,paste0(“p-curve’s estimate of effect size:\nd=”,round(dhat$minimum,3)),col=”red”)
return(dhat$minimum)
}
#Find the best fitting effect size (this also generates a diagnostic plot)
plotloss=function(t_obs,df_obs,dmin,dmax) {
loss.all=c() #Vector where results of fit for each candidate effect size are stored
di=c() #Vector where the respective effect sizes are stored
for (i in 0:((dmax-dmin)*100)) { #Do a loop considering every effect size between dmin and dmax in steps of .01
d=dmin+i/100 #What effect size are we considering?
di=c(di,d) #Add it to the vector of effect sizes
#options(warn=-1) #turn off warning because R often generates warnings when using noncentral pt() and
qt() that are inconsequential (they involve lack of precision at a degree where precision lacks practical relevance)
loss.all=c(loss.all,loss(df_obs=df_obs,t_obs=t_obs,d_est=d)) #add loss for that effect size to the vector with all losses
options(warn=0) #turn warnings back on
}
}
}
#Find the best fitting effect size (this also generates a diagnostic plot)
plotloss <- function(t_obs,df_obs,dmin,dmax) {
loss.all=c() #Vector where results of fit for each candidate effect size are stored
di=c() #Vector where the respective effect sizes are stored
}
#Find the best fitting effect size (this also generates a diagnostic plot)
plotloss <- function(t_obs,df_obs,dmin,dmax) {
loss.all=c() #Vector where results of fit for each candidate effect size are stored
di=c() #Vector where the respective effect sizes are stored
for (i in 0:((dmax-dmin)*100)) { #Do a loop considering every effect size between dmin and dmax in steps of .01
d=dmin+i/100 #What effect size are we considering?
di=c(di,d) #Add it to the vector of effect sizes
#options(warn=-1) #turn off warning because R often generates warnings when using noncentral pt() and
qt() that are inconsequential (they involve lack of precision at a degree where precision lacks practical relevance)
loss.all=c(loss.all,loss(df_obs=df_obs,t_obs=t_obs,d_est=d)) #add loss for that effect size to the vector with all losses
options(warn=0) #turn warnings back on
}
imin=match(min(loss.all),loss.all) #Find the attempted effect size that leads to smallest los overall
dstart=dmin+imin/100 #Counting from dmin, what effect size is that?
dhat=optimize(loss,c(dstart-.1,
dstart+.1), df_obs=df_obs,t_obs=t_obs) #Now optimize in the neighborhood of that effect size
#PLOT RESULTS
plot(di,loss.all,xlab=”Effect size\nCohen-d”, ylab=”Loss (D stat in KS test)”,ylim=c(0,1), main=”How well does each effect size fit?
(lower is better)”)
points(dhat$minimum,dhat$objective,pch=19,col=”red”,cex=2) #Put a red dot in the estimated effect size
#Add a label
text(dhat$minimum,dhat$objective-.08,paste0(“p-curve’s estimate of effect size:\nd=”,round(dhat$minimum,3)),col=”red”)
return(dhat$minimum)
}
#Find the best fitting effect size (this also generates a diagnostic plot)
plotloss <- function(t_obs,df_obs,dmin,dmax) {
loss.all=c() #Vector where results of fit for each candidate effect size are stored
di=c() #Vector where the respective effect sizes are stored
for (i in 0:((dmax-dmin)*100)) { #Do a loop considering every effect size between dmin and dmax in steps of .01
d=dmin+i/100 #What effect size are we considering?
di=c(di,d) #Add it to the vector of effect sizes
#options(warn=-1) #turn off warning because R often generates warnings when using noncentral pt() and
qt() that are inconsequential (they involve lack of precision at a degree where precision lacks practical relevance)
loss.all=c(loss.all,loss(df_obs=df_obs,t_obs=t_obs,d_est=d)) #add loss for that effect size to the vector with all losses
options(warn=0) #turn warnings back on
}
imin=match(min(loss.all),loss.all) #Find the attempted effect size that leads to smallest los overall
dstart=dmin+imin/100 #Counting from dmin, what effect size is that?
dhat=optimize(loss,c(dstart-.1,
dstart+.1), df_obs=df_obs,t_obs=t_obs) #Now optimize in the neighborhood of that effect size
#PLOT RESULTS
plot(di,loss.all,xlab="Effect size\nCohen-d", ylab="Loss (D stat in KS test)",ylim=c(0,1), main="How well does each effect size fit?
(lower is better)")
points(dhat$minimum,dhat$objective,pch=19,col="red",cex=2) #Put a red dot in the estimated effect size
#Add a label
text(dhat$minimum,dhat$objective-.08,paste0("p-curve’s estimate of effect size:\nd=",round(dhat$minimum,3)),col="red")
return(dhat$minimum)
}
#Find the best fitting effect size (this also generates a diagnostic plot)
plotloss <- function(t_obs,df_obs,dmin,dmax) {
loss.all=c() #Vector where results of fit for each candidate effect size are stored
di=c() #Vector where the respective effect sizes are stored
for (i in 0:((dmax-dmin)*100)) { #Do a loop considering every effect size between dmin and dmax in steps of .01
d=dmin+i/100 #What effect size are we considering?
di=c(di,d) #Add it to the vector of effect sizes
options(warn=-1) #turn off warning because R often generates warnings when using noncentral pt() and
qt() that are inconsequential (they involve lack of precision at a degree where precision lacks practical relevance)
loss.all=c(loss.all,loss(df_obs=df_obs,t_obs=t_obs,d_est=d)) #add loss for that effect size to the vector with all losses
options(warn=0) #turn warnings back on
}
imin=match(min(loss.all),loss.all) #Find the attempted effect size that leads to smallest los overall
dstart=dmin+imin/100 #Counting from dmin, what effect size is that?
dhat=optimize(loss,c(dstart-.1,
dstart+.1), df_obs=df_obs,t_obs=t_obs) #Now optimize in the neighborhood of that effect size
#PLOT RESULTS
plot(di,loss.all,xlab="Effect size\nCohen-d", ylab="Loss (D stat in KS test)",ylim=c(0,1), main="How well does each effect size fit?
(lower is better)")
points(dhat$minimum,dhat$objective,pch=19,col="red",cex=2) #Put a red dot in the estimated effect size
#Add a label
text(dhat$minimum,dhat$objective-.08,paste0("p-curve’s estimate of effect size:\nd=",round(dhat$minimum,3)),col="red")
return(dhat$minimum)
}
#Find the best fitting effect size (this also generates a diagnostic plot)
plotloss <- function(t_obs,df_obs,dmin,dmax) {
loss.all=c() #Vector where results of fit for each candidate effect size are stored
di=c() #Vector where the respective effect sizes are stored
for (i in 0:((dmax-dmin)*100)) { #Do a loop considering every effect size between dmin and dmax in steps of .01
d=dmin+i/100 #What effect size are we considering?
di=c(di,d) #Add it to the vector of effect sizes
options(warn=-1) #turn off warning because R often generates warnings when using noncentral pt() and
qt() that are inconsequential (they involve lack of precision at a degree where precision lacks practical relevance)
loss.all=c(loss.all,loss(df_obs=df_obs,t_obs=t_obs,d_est=d)) #add loss for that effect size to the vector with all losses
options(warn=0) #turn warnings back on
}
# imin=match(min(loss.all),loss.all) #Find the attempted effect size that leads to smallest los overall
# dstart=dmin+imin/100 #Counting from dmin, what effect size is that?
# dhat=optimize(loss,c(dstart-.1,
#                      dstart+.1), df_obs=df_obs,t_obs=t_obs) #Now optimize in the neighborhood of that effect size
# #PLOT RESULTS
# plot(di,loss.all,xlab="Effect size\nCohen-d", ylab="Loss (D stat in KS test)",ylim=c(0,1), main="How well does each effect size fit?
#        (lower is better)")
# points(dhat$minimum,dhat$objective,pch=19,col="red",cex=2) #Put a red dot in the estimated effect size
# #Add a label
# text(dhat$minimum,dhat$objective-.08,paste0("p-curve’s estimate of effect size:\nd=",round(dhat$minimum,3)),col="red")
# return(dhat$minimum)
}
#Find the best fitting effect size (this also generates a diagnostic plot)
plotloss <- function(t_obs,df_obs,dmin,dmax) {
loss.all=c() #Vector where results of fit for each candidate effect size are stored
di=c() #Vector where the respective effect sizes are stored
for (i in 0:((dmax-dmin)*100)) { #Do a loop considering every effect size between dmin and dmax in steps of .01
d=dmin+i/100 #What effect size are we considering?
di=c(di,d) #Add it to the vector of effect sizes
options(warn=-1) #turn off warning because R often generates warnings when using noncentral pt() and
# qt() that are inconsequential (they involve lack of precision at a degree where precision lacks practical relevance)
# loss.all=c(loss.all,loss(df_obs=df_obs,t_obs=t_obs,d_est=d)) #add loss for that effect size to the vector with all losses
# options(warn=0) #turn warnings back on
}
# imin=match(min(loss.all),loss.all) #Find the attempted effect size that leads to smallest los overall
# dstart=dmin+imin/100 #Counting from dmin, what effect size is that?
# dhat=optimize(loss,c(dstart-.1,
#                      dstart+.1), df_obs=df_obs,t_obs=t_obs) #Now optimize in the neighborhood of that effect size
# #PLOT RESULTS
# plot(di,loss.all,xlab="Effect size\nCohen-d", ylab="Loss (D stat in KS test)",ylim=c(0,1), main="How well does each effect size fit?
#        (lower is better)")
# points(dhat$minimum,dhat$objective,pch=19,col="red",cex=2) #Put a red dot in the estimated effect size
# #Add a label
# text(dhat$minimum,dhat$objective-.08,paste0("p-curve’s estimate of effect size:\nd=",round(dhat$minimum,3)),col="red")
# return(dhat$minimum)
}
#Find the best fitting effect size (this also generates a diagnostic plot)
plotloss <- function(t_obs,df_obs,dmin,dmax) {
loss.all=c() #Vector where results of fit for each candidate effect size are stored
di=c() #Vector where the respective effect sizes are stored
for (i in 0:((dmax-dmin)*100)) { #Do a loop considering every effect size between dmin and dmax in steps of .01
d=dmin+i/100 #What effect size are we considering?
di=c(di,d) #Add it to the vector of effect sizes
options(warn=-1) #turn off warning because R often generates warnings when using noncentral pt() and qt() that are inconsequential (they involve lack of precision at a degree where precision lacks practical relevance)
loss.all=c(loss.all,loss(df_obs=df_obs,t_obs=t_obs,d_est=d)) #add loss for that effect size to the vector with all losses
# options(warn=0) #turn warnings back on
}
# imin=match(min(loss.all),loss.all) #Find the attempted effect size that leads to smallest los overall
# dstart=dmin+imin/100 #Counting from dmin, what effect size is that?
# dhat=optimize(loss,c(dstart-.1,
#                      dstart+.1), df_obs=df_obs,t_obs=t_obs) #Now optimize in the neighborhood of that effect size
# #PLOT RESULTS
# plot(di,loss.all,xlab="Effect size\nCohen-d", ylab="Loss (D stat in KS test)",ylim=c(0,1), main="How well does each effect size fit?
#        (lower is better)")
# points(dhat$minimum,dhat$objective,pch=19,col="red",cex=2) #Put a red dot in the estimated effect size
# #Add a label
# text(dhat$minimum,dhat$objective-.08,paste0("p-curve’s estimate of effect size:\nd=",round(dhat$minimum,3)),col="red")
# return(dhat$minimum)
}
#Find the best fitting effect size (this also generates a diagnostic plot)
plotloss <- function(t_obs,df_obs,dmin,dmax) {
loss.all=c() #Vector where results of fit for each candidate effect size are stored
di=c() #Vector where the respective effect sizes are stored
for (i in 0:((dmax-dmin)*100)) { #Do a loop considering every effect size between dmin and dmax in steps of .01
d=dmin+i/100 #What effect size are we considering?
di=c(di,d) #Add it to the vector of effect sizes
options(warn=-1) #turn off warning because R often generates warnings when using noncentral pt() and qt() that are inconsequential (they involve lack of precision at a degree where precision lacks practical relevance)
loss.all=c(loss.all,loss(df_obs=df_obs,t_obs=t_obs,d_est=d)) #add loss for that effect size to the vector with all losses
options(warn=0) #turn warnings back on
}
# imin=match(min(loss.all),loss.all) #Find the attempted effect size that leads to smallest los overall
# dstart=dmin+imin/100 #Counting from dmin, what effect size is that?
# dhat=optimize(loss,c(dstart-.1,
#                      dstart+.1), df_obs=df_obs,t_obs=t_obs) #Now optimize in the neighborhood of that effect size
# #PLOT RESULTS
# plot(di,loss.all,xlab="Effect size\nCohen-d", ylab="Loss (D stat in KS test)",ylim=c(0,1), main="How well does each effect size fit?
#        (lower is better)")
# points(dhat$minimum,dhat$objective,pch=19,col="red",cex=2) #Put a red dot in the estimated effect size
# #Add a label
# text(dhat$minimum,dhat$objective-.08,paste0("p-curve’s estimate of effect size:\nd=",round(dhat$minimum,3)),col="red")
# return(dhat$minimum)
}
#Find the best fitting effect size (this also generates a diagnostic plot)
plotloss <- function(t_obs,df_obs,dmin,dmax) {
loss.all=c() #Vector where results of fit for each candidate effect size are stored
di=c() #Vector where the respective effect sizes are stored
for (i in 0:((dmax-dmin)*100)) { #Do a loop considering every effect size between dmin and dmax in steps of .01
d=dmin+i/100 #What effect size are we considering?
di=c(di,d) #Add it to the vector of effect sizes
options(warn=-1) #turn off warning because R often generates warnings when using noncentral pt() and qt() that are inconsequential (they involve lack of precision at a degree where precision lacks practical relevance)
loss.all=c(loss.all,loss(df_obs=df_obs,t_obs=t_obs,d_est=d)) #add loss for that effect size to the vector with all losses
options(warn=0) #turn warnings back on
}
imin=match(min(loss.all),loss.all) #Find the attempted effect size that leads to smallest los overall
dstart=dmin+imin/100 #Counting from dmin, what effect size is that?
dhat=optimize(loss,c(dstart-.1,
dstart+.1), df_obs=df_obs,t_obs=t_obs) #Now optimize in the neighborhood of that effect size
#PLOT RESULTS
plot(di,loss.all,xlab="Effect size\nCohen-d", ylab="Loss (D stat in KS test)",ylim=c(0,1), main="How well does each effect size fit?
(lower is better)")
points(dhat$minimum,dhat$objective,pch=19,col="red",cex=2) #Put a red dot in the estimated effect size
#Add a label
text(dhat$minimum,dhat$objective-.08,paste0("p-curve’s estimate of effect size:\nd=",round(dhat$minimum,3)),col="red")
return(dhat$minimum)
}
#Example
t_obs= c(1.7, 2.8, -3.1, 2.4) # include one p > .05 and one negative t-value to highlight how we treat those
df_obs=c(44, 75, 125, 200)
plotloss(t_obs=t_obs,df_obs=df_obs,dmin=-1,dmax=1)
t_obs=abs(t_obs) #Take absolute value of t-value (p-curve assumes same sign and/or sign does not matter)
p_obs=2*(1-pt(t_obs,df=df_obs)) #Compute p-values of each t in t_obs so as to keep only p<.05 results
t.sig=subset(t_obs,p_obs<.05) #Significant t-values
t-sig
t.sig
df_obs
p_obs
tc
tc=qt(.975,df.sig) #Compute critical t-value to get p=.05
df.sig=subset(df_obs,p_obs<.05) #d.f. associated with significant t.values
ncp_est=sqrt((df.sig+2)/4)*d_est #Compute noncentrality parameter for that sample size and candidate effect size
t_obs=abs(t_obs) #Take absolute value of t-value (p-curve assumes same sign and/or sign does not matter)
p_obs=2*(1-pt(t_obs,df=df_obs)) #Compute p-values of each t in t_obs so as to keep only p<.05 results
t.sig=subset(t_obs,p_obs<.05) #Significant t-values
df.sig=subset(df_obs,p_obs<.05) #d.f. associated with significant t.values
ncp_est=sqrt((df.sig+2)/4)*d_est #Compute noncentrality parameter for that sample size and candidate effect size
#Example
t_obs= c(1.7, 2.8, -3.1, 2.4) # include one p > .05 and one negative t-value to highlight how we treat those
df_obs=c(44, 75, 125, 200)
plotloss(t_obs=t_obs,df_obs=df_obs,dmin=-1,dmax=1)
i <- 0
d_est=dmin+i/100 #What effect size are we considering?
d_min <- -1
d_max <1
d_max <-1
d_est=dmin+i/100 #What effect size are we considering?
dmax <-1
dmin <- -1
d_est=dmin+i/100 #What effect size are we considering?
di=c(di,d) #Add it to the vector of effect sizes
loss.all=c() #Vector where results of fit for each candidate effect size are stored
di=c() #Vector where the respective effect sizes are stored
d_est=dmin+i/100 #What effect size are we considering?
di=c(di,d) #Add it to the vector of effect sizes
di=c(di,d_est) #Add it to the vector of effect sizes
loss.all=c(loss.all,loss(df_obs=df_obs,t_obs=t_obs,d_est=d)) #add loss for that effect size to the vector with all losses
loss.all=c(loss.all,loss(df_obs=df_obs,t_obs=t_obs,d_est=d_est)) #add loss for that effect size to the vector with all losses
loss.all
t_obs=abs(t_obs) #Take absolute value of t-value (p-curve assumes same sign and/or sign does not matter)
p_obs=2*(1-pt(t_obs,df=df_obs)) #Compute p-values of each t in t_obs so as to keep only p<.05 results
t.sig=subset(t_obs,p_obs<.05) #Significant t-values
df.sig=subset(df_obs,p_obs<.05) #d.f. associated with significant t.values
ncp_est=sqrt((df.sig+2)/4)*d_est #Compute noncentrality parameter for that sample size and candidate effect size
tc=qt(.975,df.sig) #Compute critical t-value to get p=.05
tc
t_obs
tc=qt(.975,df.sig) #Compute critical t-value to get p=.05
power_est=1-pt(tc,df.sig,ncp_est) #Compute power for obtaining that t-value or bigger, given the noncentrality parameter
power_est
t_obs=abs(t_obs) #Take absolute value of t-value (p-curve assumes same sign and/or sign does not matter)
p_obs=2*(1-pt(t_obs,df=df_obs)) #Compute p-values of each t in t_obs so as to keep only p<.05 results
t.sig=subset(t_obs,p_obs<.05) #Significant t-values
df.sig=subset(df_obs,p_obs<.05) #d.f. associated with significant t.values
ncp_est=sqrt((df.sig+2)/4)*d_est #Compute noncentrality parameter for that sample size and candidate effect size
tc=qt(.975,df.sig) #Compute critical t-value to get p=.05
power_est=1-pt(tc,df.sig,ncp_est) #Compute power for obtaining that t-value or bigger, given the noncentrality parameter
power_est
t_obs
p_larger=pt(t.sig,df=df.sig,ncp=ncp_est) #Probability of obtaining a t-value bigger than the one that is observed (this is a vector)
p_larger
t.sig
df.sig
ncp_est
d_est
ppr=(p_larger-(1-power_est))/power_est #Conditional probability of larger t-value given that it is p<.05, pp-values
ppr
p_larger
p_obs
ncp_est
t.sig
p_larger
help(pt)
t.sug
t.sig
d_est = 1
t_obs=abs(t_obs) #Take absolute value of t-value (p-curve assumes same sign and/or sign does not matter)
p_obs=2*(1-pt(t_obs,df=df_obs)) #Compute p-values of each t in t_obs so as to keep only p<.05 results
t.sig=subset(t_obs,p_obs<.05) #Significant t-values
df.sig=subset(df_obs,p_obs<.05) #d.f. associated with significant t.values
ncp_est=sqrt((df.sig+2)/4)*d_est #Compute noncentrality parameter for that sample size and candidate effect size
tc=qt(.975,df.sig) #Compute critical t-value to get p=.05
power_est=1-pt(tc,df.sig,ncp_est) #Compute power for obtaining that t-value or bigger, given the noncentrality parameter
p_larger=pt(t.sig,df=df.sig,ncp=ncp_est) #Probability of obtaining a t-value bigger than the one that is observed (this is a vector)
p_larger
ppr
power_est
t_obs=abs(t_obs) #Take absolute value of t-value (p-curve assumes same sign and/or sign does not matter)
p_obs=2*(1-pt(t_obs,df=df_obs)) #Compute p-values of each t in t_obs so as to keep only p<.05 results
t.sig=subset(t_obs,p_obs<.05) #Significant t-values
df.sig=subset(df_obs,p_obs<.05) #d.f. associated with significant t.values
ncp_est=sqrt((df.sig+2)/4)*d_est #Compute noncentrality parameter for that sample size and candidate effect size
tc=qt(.975,df.sig) #Compute critical t-value to get p=.05
power_est=1-pt(tc,df.sig,ncp_est) #Compute power for obtaining that t-value or bigger, given the noncentrality parameter
p_larger=pt(t.sig,df=df.sig,ncp=ncp_est) #Probability of obtaining a t-value bigger than the one that is observed (this is a vector)
ppr=(p_larger-(1-power_est))/power_est #Conditional probability of larger t-value given that it is p<.05, pp-values
KSD=ks.test(ppr,punif)$statistic #Kolmogorov Smirnov test on that vector against the theoretical U[0,1] distribution
KSD
power_est
p_larger
1-power_est
qt(.975,df = 38)
qt(.98,df = 38)
x1 <- qt(.995,df = 38)
x1
x5 <- qt(.975,df = 38)
x5
1-pt(x1,df=38,2)
1-pt(x5,df=38,2)
x5
x4
x1
qt(0.975,38,2)
x6 <- qt(0.975,38,2)
1-pt(x6,df=38,2)
1-pt(x6,df=38,2)
1-pt(x5,df=38,2)
1-pt(x1,df=38,2)
1-pt(x6,df=38,2)
help(ppr)
help(ks.test)
